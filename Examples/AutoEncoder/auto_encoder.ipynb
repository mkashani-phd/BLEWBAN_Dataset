{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, pickle\n",
    "import numpy as np\n",
    "import pymongo\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import src.IQ as IQ\n",
    "\n",
    "\n",
    "myclient = pymongo.MongoClient(\"mongodb://127.0.0.1:27017/admin\")\n",
    "BLE = myclient[\"BLE\"]\n",
    "\n",
    "def query(collection, filter:dict, addFrameColumn=True):\n",
    "    df =  pd.DataFrame(list(collection.find(filter)))\n",
    "    if addFrameColumn:\n",
    "        df['frame'] = df.apply(lambda x: x['I'] + np.dot(x['Q'],1j), axis=1)\n",
    "    return df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-16 09:13:09.064553: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-16 09:13:09.304954: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-16 09:13:09.304978: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-16 09:13:09.306182: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-16 09:13:09.397620: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-16 09:13:22.634855: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-16 09:13:22.654046: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-16 09:13:22.654148: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-16 09:13:22.655990: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-16 09:13:22.656074: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-16 09:13:22.656141: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-16 09:13:23.358785: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-16 09:13:23.358888: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-16 09:13:23.358961: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-16 09:13:23.359019: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4679 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2023-11-16 09:13:26.881757: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-11-16 09:13:27.200152: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fed7c24f9c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-11-16 09:13:27.200168: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Ti, Compute Capability 8.6\n",
      "2023-11-16 09:13:27.205869: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-11-16 09:13:27.333655: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8700\n",
      "2023-11-16 09:13:27.389621: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: early stopping\n",
      "latent dim: 1 loss value is:  0.44025591015815735\n",
      "Epoch 132: early stopping\n",
      "latent dim: 1 loss value is:  0.410152405500412\n",
      "Epoch 141: early stopping\n",
      "latent dim: 1 loss value is:  0.40381601452827454\n",
      "Epoch 82: early stopping\n",
      "latent dim: 2 loss value is:  0.1825215369462967\n",
      "Epoch 103: early stopping\n",
      "latent dim: 2 loss value is:  0.14324229955673218\n",
      "Epoch 137: early stopping\n",
      "latent dim: 2 loss value is:  0.14806343615055084\n",
      "Epoch 102: early stopping\n",
      "latent dim: 3 loss value is:  0.16430790722370148\n",
      "Epoch 103: early stopping\n",
      "latent dim: 3 loss value is:  0.1675473302602768\n",
      "Epoch 262: early stopping\n",
      "latent dim: 3 loss value is:  0.14174175262451172\n",
      "Epoch 65: early stopping\n",
      "latent dim: 4 loss value is:  0.1513148844242096\n",
      "Epoch 95: early stopping\n",
      "latent dim: 4 loss value is:  0.16627444326877594\n",
      "Epoch 167: early stopping\n",
      "latent dim: 4 loss value is:  0.1443399339914322\n",
      "Epoch 102: early stopping\n",
      "latent dim: 5 loss value is:  0.1554136574268341\n",
      "Epoch 131: early stopping\n",
      "latent dim: 5 loss value is:  0.134348064661026\n",
      "Epoch 260: early stopping\n",
      "latent dim: 5 loss value is:  0.10836157947778702\n",
      "Epoch 145: early stopping\n",
      "latent dim: 6 loss value is:  0.12853001058101654\n",
      "Epoch 148: early stopping\n",
      "latent dim: 6 loss value is:  0.1198834553360939\n",
      "Epoch 215: early stopping\n",
      "latent dim: 6 loss value is:  0.11936043947935104\n",
      "Epoch 76: early stopping\n",
      "latent dim: 7 loss value is:  0.14405040442943573\n",
      "Epoch 101: early stopping\n",
      "latent dim: 7 loss value is:  0.15140146017074585\n",
      "Epoch 138: early stopping\n",
      "latent dim: 7 loss value is:  0.13282510638237\n",
      "Epoch 65: early stopping\n",
      "latent dim: 8 loss value is:  0.182733952999115\n",
      "Epoch 130: early stopping\n",
      "latent dim: 8 loss value is:  0.1524902582168579\n",
      "Epoch 148: early stopping\n",
      "latent dim: 8 loss value is:  0.10813075304031372\n",
      "Epoch 121: early stopping\n",
      "latent dim: 9 loss value is:  0.13558587431907654\n",
      "Epoch 150: early stopping\n",
      "latent dim: 9 loss value is:  0.12268948554992676\n",
      "Epoch 200: early stopping\n",
      "latent dim: 9 loss value is:  0.13163280487060547\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "import threading\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# We will start with a size that is approximately half of the input size\n",
    "def run(encoding_dim,input_tensor_features,output_tensor_features, plot = False):\n",
    "    tf.keras.backend.clear_session()\n",
    "    input_shape = input_tensor_features[0].shape[0]\n",
    "    output_shape = output_tensor_features[0].shape[0]\n",
    "    input_data = Input(shape=(input_shape,))\n",
    "    encoded = Dense(encoding_dim + (input_shape - encoding_dim )//2, activation='relu')(input_data)\n",
    "    encoded = Dense(encoding_dim + (input_shape - encoding_dim )//4, activation='tanh')(encoded)\n",
    "    \n",
    "    encoded = Dense(encoding_dim, activation='linear')(encoded) # kernel_regularizer=l1(0.01)\n",
    "\n",
    "\n",
    "    decoded = Dense(encoding_dim + (output_shape - encoding_dim )//4, activation='tanh')(encoded) \n",
    "    decoded = Dense(encoding_dim + (output_shape - encoding_dim )//2, activation='relu')(decoded)\n",
    "    decoded = Dense(output_shape, activation='linear')(decoded)\n",
    "\n",
    "    # This model maps an input to its reconstruction\n",
    "    autoencoder = Model(input_data, decoded)\n",
    "\n",
    "    # This model maps an input to its encoded representation\n",
    "    # encoder = Model(input_data, encoded)\n",
    "\n",
    "    # Compile the model\n",
    "    autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "    # Display the architecture of the autoencoder\n",
    "    # with lock:\n",
    "    #     autoencoder.summary()\n",
    "\n",
    "    # Train the autoencoder\n",
    "    # We will use new_csv_features as both the input and the target since it's an autoencoder\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=40, verbose=1)\n",
    "\n",
    "    # Train the autoencoder and save the history\n",
    "    history = autoencoder.fit(input_tensor_features, output_tensor_features,\n",
    "                            epochs=2048,\n",
    "                            batch_size=16,\n",
    "                            shuffle=True,\n",
    "                            validation_split=0.2,\n",
    "                            callbacks=[early_stopping],\n",
    "                            verbose=0,\n",
    "                            )  # Set verbose to 1 for progress output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    best_loss = min(history.history['val_loss']) # this is the raining loss\n",
    "    with lock:\n",
    "        try:\n",
    "            best_losses[encoding_dim] = min(best_loss,best_losses[encoding_dim])\n",
    "        except:\n",
    "            best_losses[encoding_dim] = best_loss\n",
    "            \n",
    "    print(\"latent dim:\", encoding_dim, \"loss value is: \", best_loss)\n",
    "\n",
    "    if plot:\n",
    "        lock.acquire()\n",
    "        plt.figure(figsize=(10, 2),dpi=80)\n",
    "        plt.plot(output_tensor_features[199], label='original')\n",
    "        plt.plot(autoencoder(input_tensor_features)[199], label='reconstructed')\n",
    "        plt.title('Latnet dim:'+ str(encoding_dim)+\", best loss:\"+str(best_loss)) \n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        # plt.figure(figsize=(20, 4))\n",
    "        # for i in range (1,len(autoencoder.layers)-1):\n",
    "        # # Assuming 'autoencoder' is your trained model and 'layer_index' is the index of the layer you want to analyze\n",
    "        #     weights = autoencoder.layers[i].get_weights()[0]  # 0 for weights, 1 for biases\n",
    "\n",
    "        #     # Plotting the weights as a heatmap\n",
    "            \n",
    "        #     plt.subplot(1,len(autoencoder.layers)-2,i)\n",
    "        #     plt.imshow(weights, cmap='viridis', aspect='auto')\n",
    "        #     plt.colorbar()\n",
    "\n",
    "        # plt.show()\n",
    "        # plt.close()\n",
    "\n",
    "        # plt.figure(figsize=(10, 16))\n",
    "        # for i in range (1,len(autoencoder.layers)-1):\n",
    "        # # Assuming 'autoencoder' is your trained model and 'layer_index' is the index of the layer you want to analyze\n",
    "        #     weights = autoencoder.layers[i].get_weights()[0]  # 0 for weights, 1 for biases\n",
    "        #     biases = autoencoder.layers[i].get_weights()[1]  # 0 for weights, 1 for biases\n",
    "        #     # Histogram of the weight values\n",
    "        #     plt.subplot(2*(len(autoencoder.layers)-2),1,i*2-1)\n",
    "        #     plt.plot(weights.flatten().tolist(), label='weights')\n",
    "        #     plt.plot(np.linspace(0,len(weights.flatten()),len(biases.flatten())),biases.flatten().tolist(), label='biases')\n",
    "        #     plt.legend()\n",
    "        #     plt.subplot(2*(len(autoencoder.layers)-2),2,i*4-1)\n",
    "        #     plt.hist(weights.flatten(), bins=50)\n",
    "\n",
    "            \n",
    "        #     # Histogram of the biases values\n",
    "        #     plt.subplot(2*(len(autoencoder.layers)-2),2,i*4)\n",
    "        #     plt.hist(biases.flatten(), bins=50)\n",
    "\n",
    "        # plt.show()\n",
    "        # plt.close()\n",
    "\n",
    "        lock.release()\n",
    "    tf.keras.backend.clear_session()\n",
    "    clear_session()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "scaler2 = MinMaxScaler(feature_range=(0, 1))\n",
    "# Assuming 'new_csv_features' is your data\n",
    "def normalized(row):\n",
    "    row = np.array(row)\n",
    "    return scaler2.fit_transform(scaler.fit_transform(row.reshape(-1, 1)))\n",
    "\n",
    "def fft_normalized(row):\n",
    "    row = np.array(row)\n",
    "    temp = np.fft.fft(row)[0:len(row)//2 + 1]\n",
    "\n",
    "    amp = normalized(np.abs(temp))\n",
    "    filtering = amp > np.average(amp)*.05\n",
    "    angle = normalized(np.angle(temp))\n",
    "\n",
    "    angle = angle[filtering] \n",
    "    angle = np.concatenate([angle,np.zeros(len(row)-len(angle))])\n",
    "    amp = amp[filtering] \n",
    "    amp = np.concatenate([amp,np.zeros(len(row)-len(amp))])\n",
    "    \n",
    "    return np.concatenate([amp,angle])\n",
    "\n",
    "lock = threading.Lock()\n",
    "\n",
    "\n",
    "filtering = {''}\n",
    "df = query(BLE['onBody'], {'pos':'static','antenna_side':'left'})\n",
    "min_length = df['frame'].apply(len).min()\n",
    "df['frame'] = df['frame'].apply(lambda x: x[:2000])\n",
    "print(len(df['frame'][0]))\n",
    "print(type(df['frame'][0]))\n",
    "\n",
    "iq = IQ.IQ(Fc=2439810000+.1e4)\n",
    "\n",
    "def configCreator(downSampleRate = 1, cutoff = 1e6):\n",
    "    downSampleRate= max(downSampleRate, 1)\n",
    "    return {                                      \n",
    "            # iq.gradient:{},\n",
    "            iq.unwrapPhase:{},\n",
    "            iq.phase:{}, \n",
    "            # iq.butter:{'Fs': iq.Fs/downSampleRate, \"cutoff\": cutoff},\n",
    "            iq.downSample:{'downSampleRate':downSampleRate, \"shift\": 0},\n",
    "            iq.demodulate:{'Fs': iq.Fs},\n",
    "           } \n",
    "\n",
    "methods = configCreator(downSampleRate=  4)\n",
    "df['data'] = iq.apply(methods = methods, frame = df)\n",
    "####################################################################################\n",
    "# df['normalized'] = df['predictor'].apply(lambda x: normalized(x))\n",
    "\n",
    "df['normalized'] = df['data'].apply(lambda x: fft_normalized(x))\n",
    "input_tensor_features = tf.convert_to_tensor(df['normalized'].tolist())\n",
    "\n",
    "df['normalized'] = df['data']#.apply(lambda x: normalized(x))\n",
    "output_tensor_features = tf.convert_to_tensor(df['normalized'].tolist())\n",
    "\n",
    "data_shape = len(df['data'][0])\n",
    "#####################################################################################\n",
    "\n",
    "best_losses = {}\n",
    "batch_size = 3\n",
    "for batch in range(0,int(np.ceil(data_shape/batch_size))):\n",
    "    # try:\n",
    "    #     run(batch,output_tensor_features,output_tensor_features,True)\n",
    "    # except:\n",
    "    #     continue\n",
    "    threads = []\n",
    "    # for i in range(batch_size * batch+1, min(batch_size * batch+batch_size+1,data_shape)):\n",
    "    for i in range(batch_size): # runnig the code batch_size times in parallel to find the miunimum loss\n",
    "        threads.append(threading.Thread(target=run, args=(batch+1,output_tensor_features,output_tensor_features,False, )))\n",
    "        threads[-1].start()\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "\n",
    "best_losses = dict(sorted(best_losses.items()))\n",
    "lossDF = pd.DataFrame(best_losses.values(),index=best_losses.keys())\n",
    "lossDF.plot()\n",
    "lossDF.to_csv('loss.csv')\n",
    "plt.xlabel('Latent dimension')\n",
    "plt.ylabel('Best loss')\n",
    "plt.title('dataset: onBody static left')\n",
    "plt.savefig('res/'+\"onBody static left\"+'.png')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "row =np.random.rand(10) \n",
    "\n",
    "filename = files[3]\n",
    "print(filename)\n",
    "f = open(dataDir+filename, 'rb')\n",
    "data_tuples = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "df = pd.DataFrame(data_tuples, columns=['predictor', 'label'])\n",
    "\n",
    "def fft_normalized(row):\n",
    "    temp = np.fft.fft(row)[0:len(row)//2 + 1]\n",
    "    amp = normalized(np.abs(temp))\n",
    "    filtering = amp > np.max(amp)*0.05 \n",
    "\n",
    "    angle = normalized(np.angle(temp))\n",
    "    angle = angle[filtering]\n",
    "    amp = amp[filtering]\n",
    "\n",
    "    return np.concatenate([amp,angle])\n",
    "\n",
    "\n",
    "df['normalized'] = df['predictor'].apply(lambda x: fft_normalized(x))\n",
    "tf.convert_to_tensor( df['normalized'].to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_losses = dict(sorted(best_losses.items()))\n",
    "# lossDF = pd.DataFrame(best_losses.values(),index=best_losses.keys())\n",
    "\n",
    "def read_loss(file :str | os.DirEntry):\n",
    "    if type(file) is str:\n",
    "        if file.endswith(\".csv\"):\n",
    "            lossDF = pd.read_csv(file,index_col=0)\n",
    "            return lossDF\n",
    "    else:\n",
    "        if file.name.endswith(\".csv\"):\n",
    "            lossDF = pd.read_csv(file.path,index_col=0)\n",
    "            return lossDF\n",
    "\n",
    "def plot_loss(file):\n",
    "        read_loss(file)\n",
    "        lossDF.plot()\n",
    "        plt.xlabel('Latent dimension')\n",
    "        plt.ylabel('Best loss')\n",
    "        plt.title('dataset: '+file.name)\n",
    "        plt.savefig('res/'+file.name+'.png')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "# for file in os.scandir('res/RAE-_l2-0.001/'):\n",
    "df = read_loss(\"loss_dataset1.pkl.csv\")\n",
    "import scipy.signal as signal\n",
    "\n",
    "def smooth(x,window_len=11,window='hanning'):\n",
    "    if x.ndim != 1:\n",
    "        raise ValueError(\"smooth only accepts 1 dimension arrays.\")\n",
    "    if x.size < window_len:\n",
    "        raise ValueError(\"Input vector needs to be bigger than window size.\")\n",
    "    if window_len<3:\n",
    "        return x\n",
    "    if not window in ['flat', 'hanning', 'hamming', 'bartlett', 'blackman']:\n",
    "        raise ValueError(f\"Window is on of '{window}'\")\n",
    "    s=np.r_[x[window_len-1:0:-1],x,x[-2:-window_len-1:-1]]\n",
    "    #print(len(s))\n",
    "    if window == 'flat': #moving average\n",
    "        w=np.ones(window_len,'d')\n",
    "    else:\n",
    "        w=eval(f'np.{window}({window_len})')\n",
    "    y=np.convolve(w/w.sum(),s,mode='valid')\n",
    "    return y\n",
    "\n",
    "plt.plot(smooth(df['0'].to_numpy(),window_len=5, window='hamming'))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
