{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## you can run pip3 install -r requirements.txt to install all the packages\n",
    "## but you need to install tensorflow or pytorch or keras manually\n",
    "import pickle\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import threading, os, sys\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '-1' # try to use CPU only\n",
    "\n",
    "# addin path to import IQ module\n",
    "sys.path.append('../../')\n",
    "import src.IQ as IQ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# myclient = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "myclient = pymongo.MongoClient(\"mongodb://test:12345678910111213@SG-pine-beat-9444-57323.servers.mongodirector.com:27017/BLE\")\n",
    "BLE = myclient[\"BLE\"]\n",
    "\n",
    "def query(collection, filter:dict, addFrameColumn=True):\n",
    "    df =  pd.DataFrame(list(collection.find(filter)))\n",
    "    if addFrameColumn:\n",
    "        df['frame'] = df.apply(lambda x: x['I'] + np.dot(x['Q'],1j), axis=1)\n",
    "    return df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "iq = IQ.IQ(Fc=2439810000+.1e4)\n",
    "\n",
    "def configCreator(downSampleRate = 10, cutoff = 1e6):\n",
    "    downSampleRate= max(downSampleRate, 1)\n",
    "    return {                                      \n",
    "            iq.butter:{'Fs': iq.Fs/downSampleRate, \"cutoff\": cutoff},\n",
    "            iq.downSample:{'downSampleRate':downSampleRate, \"shift\": 0},\n",
    "            iq.demodulate:{'Fs': iq.Fs},\n",
    "           }\n",
    "\n",
    "downSampleRate = 1\n",
    "methods = configCreator(downSampleRate=  downSampleRate)\n",
    "\n",
    "with open('data/E7.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "df = pd.DataFrame(data)\n",
    "df['frame'] = df.apply(lambda x: x['I'] + np.dot(x['Q'],1j), axis=1)\n",
    "df['data'] = iq.apply(methods = methods, frame = df)\n",
    "\n",
    "############### Defing and normalizing the input #############\n",
    "# df['normalized_input_feature'] = df['data'].apply(lambda x: fft_normalized(x, threshold = 1))\n",
    "df['normalized_input_feature'] = df['data'].apply(lambda x: x[0:2000//downSampleRate])\n",
    "# df['normalized_input_feature_realImage'] = df['normalized_input_feature'].apply(lambda x: np.concatenate([np.real(x[0:2000]), np.imag(x[0:2000])]).reshape(2,-1))\n",
    "##################################################################################\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['normalized_input_feature'], df['dvc'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2348,), (588,), (2348,), (588,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "X_train = tf.convert_to_tensor(X_train.tolist())\n",
    "X_test =  tf.convert_to_tensor(X_test.tolist())\n",
    "y_train =  tf.convert_to_tensor(y_train.tolist())\n",
    "y_test = tf.convert_to_tensor(y_test.tolist())\n",
    "\n",
    "y_train_encoded = to_categorical(y_train)\n",
    "y_test_encoded = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_11 (ComplexInput)     [(None, 2000, 1)]         0         \n",
      "                                                                 \n",
      " complex_conv1d_20 (Complex  (None, 1873, 2)           516       \n",
      " Conv1D)                                                         \n",
      "                                                                 \n",
      " complex_avg_pooling1d_20 (  (None, 936, 2)            0         \n",
      " ComplexAvgPooling1D)                                            \n",
      "                                                                 \n",
      " complex_conv1d_21 (Complex  (None, 809, 2)            1028      \n",
      " Conv1D)                                                         \n",
      "                                                                 \n",
      " complex_avg_pooling1d_21 (  (None, 404, 2)            0         \n",
      " ComplexAvgPooling1D)                                            \n",
      "                                                                 \n",
      " complex_flatten_10 (Comple  (None, 808)               0         \n",
      " xFlatten)                                                       \n",
      "                                                                 \n",
      " complex_dropout_5 (Complex  (None, 808)               0         \n",
      " Dropout)                                                        \n",
      "                                                                 \n",
      " complex_dense_10 (ComplexD  (None, 100)               161800    \n",
      " ense)                                                           \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 13)                1313      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 174757 (682.64 KB)\n",
      "Trainable params: 174757 (682.64 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/512\n",
      "3/3 [==============================] - 2s 117ms/step - loss: 3.1905 - accuracy: 0.1014 - val_loss: 3.0597 - val_accuracy: 0.1105\n",
      "Epoch 2/512\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 3.0111 - accuracy: 0.1227 - val_loss: 2.9194 - val_accuracy: 0.1122\n",
      "Epoch 3/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 2.8815 - accuracy: 0.1312 - val_loss: 2.7684 - val_accuracy: 0.1344\n",
      "Epoch 4/512\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 2.7659 - accuracy: 0.1508 - val_loss: 2.7057 - val_accuracy: 0.1122\n",
      "Epoch 5/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 2.7084 - accuracy: 0.1342 - val_loss: 2.6874 - val_accuracy: 0.1224\n",
      "Epoch 6/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 2.6776 - accuracy: 0.1371 - val_loss: 2.6554 - val_accuracy: 0.1395\n",
      "Epoch 7/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 2.6408 - accuracy: 0.1486 - val_loss: 2.6191 - val_accuracy: 0.1241\n",
      "Epoch 8/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 2.6124 - accuracy: 0.1333 - val_loss: 2.5762 - val_accuracy: 0.1327\n",
      "Epoch 9/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 2.5787 - accuracy: 0.1584 - val_loss: 2.5434 - val_accuracy: 0.1480\n",
      "Epoch 10/512\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 2.5413 - accuracy: 0.1644 - val_loss: 2.5152 - val_accuracy: 0.1429\n",
      "Epoch 11/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 2.5108 - accuracy: 0.1746 - val_loss: 2.4922 - val_accuracy: 0.1616\n",
      "Epoch 12/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 2.4791 - accuracy: 0.1861 - val_loss: 2.4675 - val_accuracy: 0.1667\n",
      "Epoch 13/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 2.4514 - accuracy: 0.1891 - val_loss: 2.4411 - val_accuracy: 0.1888\n",
      "Epoch 14/512\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 2.4361 - accuracy: 0.2049 - val_loss: 2.3737 - val_accuracy: 0.2007\n",
      "Epoch 15/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 2.3967 - accuracy: 0.2206 - val_loss: 2.3579 - val_accuracy: 0.2211\n",
      "Epoch 16/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 2.3769 - accuracy: 0.2270 - val_loss: 2.3409 - val_accuracy: 0.2551\n",
      "Epoch 17/512\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 2.3578 - accuracy: 0.2436 - val_loss: 2.2842 - val_accuracy: 0.2568\n",
      "Epoch 18/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 2.2888 - accuracy: 0.2815 - val_loss: 2.2507 - val_accuracy: 0.2976\n",
      "Epoch 19/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 2.2553 - accuracy: 0.2819 - val_loss: 2.2260 - val_accuracy: 0.3180\n",
      "Epoch 20/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 2.2276 - accuracy: 0.3254 - val_loss: 2.2110 - val_accuracy: 0.3010\n",
      "Epoch 21/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 2.1796 - accuracy: 0.3292 - val_loss: 2.1223 - val_accuracy: 0.3895\n",
      "Epoch 22/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 2.1586 - accuracy: 0.3428 - val_loss: 2.0951 - val_accuracy: 0.3282\n",
      "Epoch 23/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 2.1152 - accuracy: 0.3688 - val_loss: 2.0749 - val_accuracy: 0.3350\n",
      "Epoch 24/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 2.0875 - accuracy: 0.3612 - val_loss: 2.0857 - val_accuracy: 0.3486\n",
      "Epoch 25/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 2.0738 - accuracy: 0.3807 - val_loss: 1.9878 - val_accuracy: 0.4388\n",
      "Epoch 26/512\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 2.0057 - accuracy: 0.4084 - val_loss: 1.9788 - val_accuracy: 0.4252\n",
      "Epoch 27/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 1.9741 - accuracy: 0.4170 - val_loss: 1.9639 - val_accuracy: 0.3997\n",
      "Epoch 28/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 1.9669 - accuracy: 0.4267 - val_loss: 1.9708 - val_accuracy: 0.3827\n",
      "Epoch 29/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 1.9749 - accuracy: 0.4025 - val_loss: 1.8730 - val_accuracy: 0.4286\n",
      "Epoch 30/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 1.8987 - accuracy: 0.4306 - val_loss: 1.8387 - val_accuracy: 0.4473\n",
      "Epoch 31/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 1.8680 - accuracy: 0.4651 - val_loss: 1.8918 - val_accuracy: 0.4184\n",
      "Epoch 32/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 1.8603 - accuracy: 0.4514 - val_loss: 1.8031 - val_accuracy: 0.4677\n",
      "Epoch 33/512\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 1.8181 - accuracy: 0.4830 - val_loss: 1.7896 - val_accuracy: 0.4915\n",
      "Epoch 34/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 1.7989 - accuracy: 0.4825 - val_loss: 1.8107 - val_accuracy: 0.4779\n",
      "Epoch 35/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 1.8338 - accuracy: 0.4595 - val_loss: 1.7961 - val_accuracy: 0.5051\n",
      "Epoch 36/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 1.7907 - accuracy: 0.4957 - val_loss: 1.7273 - val_accuracy: 0.4779\n",
      "Epoch 37/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 1.7292 - accuracy: 0.4881 - val_loss: 1.6908 - val_accuracy: 0.5204\n",
      "Epoch 38/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 1.6966 - accuracy: 0.5179 - val_loss: 1.6350 - val_accuracy: 0.5612\n",
      "Epoch 39/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 1.6651 - accuracy: 0.5353 - val_loss: 1.6551 - val_accuracy: 0.5306\n",
      "Epoch 40/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 1.6496 - accuracy: 0.5294 - val_loss: 1.6024 - val_accuracy: 0.5527\n",
      "Epoch 41/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 1.6395 - accuracy: 0.5302 - val_loss: 1.6610 - val_accuracy: 0.4864\n",
      "Epoch 42/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 1.6238 - accuracy: 0.5247 - val_loss: 1.5436 - val_accuracy: 0.5952\n",
      "Epoch 43/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 1.5601 - accuracy: 0.5728 - val_loss: 1.5441 - val_accuracy: 0.6054\n",
      "Epoch 44/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 1.5554 - accuracy: 0.5647 - val_loss: 1.5303 - val_accuracy: 0.5969\n",
      "Epoch 45/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 1.5380 - accuracy: 0.5835 - val_loss: 1.5212 - val_accuracy: 0.5765\n",
      "Epoch 46/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 1.4994 - accuracy: 0.5724 - val_loss: 1.4850 - val_accuracy: 0.6139\n",
      "Epoch 47/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 1.4701 - accuracy: 0.5945 - val_loss: 1.4521 - val_accuracy: 0.6344\n",
      "Epoch 48/512\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 1.4672 - accuracy: 0.5988 - val_loss: 1.4369 - val_accuracy: 0.6310\n",
      "Epoch 49/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 1.4450 - accuracy: 0.6120 - val_loss: 1.4272 - val_accuracy: 0.6395\n",
      "Epoch 50/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 1.4337 - accuracy: 0.6214 - val_loss: 1.4091 - val_accuracy: 0.6105\n",
      "Epoch 51/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 1.3902 - accuracy: 0.6239 - val_loss: 1.3767 - val_accuracy: 0.6463\n",
      "Epoch 52/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 1.3859 - accuracy: 0.6248 - val_loss: 1.4048 - val_accuracy: 0.6190\n",
      "Epoch 53/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 1.3800 - accuracy: 0.6210 - val_loss: 1.4123 - val_accuracy: 0.6122\n",
      "Epoch 54/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 1.3695 - accuracy: 0.6239 - val_loss: 1.3397 - val_accuracy: 0.6650\n",
      "Epoch 55/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 1.3574 - accuracy: 0.6367 - val_loss: 1.3573 - val_accuracy: 0.6548\n",
      "Epoch 56/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 1.3569 - accuracy: 0.6252 - val_loss: 1.3391 - val_accuracy: 0.6701\n",
      "Epoch 57/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 1.3539 - accuracy: 0.6312 - val_loss: 1.4357 - val_accuracy: 0.5799\n",
      "Epoch 58/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 1.3736 - accuracy: 0.6239 - val_loss: 1.2832 - val_accuracy: 0.6667\n",
      "Epoch 59/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 1.2976 - accuracy: 0.6508 - val_loss: 1.2687 - val_accuracy: 0.6922\n",
      "Epoch 60/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 1.2840 - accuracy: 0.6708 - val_loss: 1.2353 - val_accuracy: 0.6854\n",
      "Epoch 61/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 1.2384 - accuracy: 0.6750 - val_loss: 1.2416 - val_accuracy: 0.6718\n",
      "Epoch 62/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 1.2379 - accuracy: 0.6806 - val_loss: 1.2170 - val_accuracy: 0.6871\n",
      "Epoch 63/512\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 1.2171 - accuracy: 0.6840 - val_loss: 1.1918 - val_accuracy: 0.6922\n",
      "Epoch 64/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 1.2275 - accuracy: 0.6853 - val_loss: 1.1488 - val_accuracy: 0.7143\n",
      "Epoch 65/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 1.1843 - accuracy: 0.6976 - val_loss: 1.2051 - val_accuracy: 0.6684\n",
      "Epoch 66/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 1.1795 - accuracy: 0.6934 - val_loss: 1.1578 - val_accuracy: 0.7024\n",
      "Epoch 67/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 1.1555 - accuracy: 0.7091 - val_loss: 1.1120 - val_accuracy: 0.7262\n",
      "Epoch 68/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 1.1268 - accuracy: 0.7189 - val_loss: 1.1089 - val_accuracy: 0.7551\n",
      "Epoch 69/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 1.1303 - accuracy: 0.7193 - val_loss: 1.1503 - val_accuracy: 0.6888\n",
      "Epoch 70/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 1.1256 - accuracy: 0.7223 - val_loss: 1.0818 - val_accuracy: 0.7466\n",
      "Epoch 71/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 1.0988 - accuracy: 0.7308 - val_loss: 1.0253 - val_accuracy: 0.7959\n",
      "Epoch 72/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 1.0778 - accuracy: 0.7411 - val_loss: 1.1359 - val_accuracy: 0.6820\n",
      "Epoch 73/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 1.0953 - accuracy: 0.7172 - val_loss: 1.0419 - val_accuracy: 0.7449\n",
      "Epoch 74/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 1.0804 - accuracy: 0.7381 - val_loss: 1.0489 - val_accuracy: 0.7500\n",
      "Epoch 75/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 1.0647 - accuracy: 0.7334 - val_loss: 1.0491 - val_accuracy: 0.7347\n",
      "Epoch 76/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 1.0571 - accuracy: 0.7398 - val_loss: 1.0389 - val_accuracy: 0.7585\n",
      "Epoch 77/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 1.0485 - accuracy: 0.7491 - val_loss: 1.0338 - val_accuracy: 0.7398\n",
      "Epoch 78/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 1.0249 - accuracy: 0.7564 - val_loss: 0.9762 - val_accuracy: 0.7908\n",
      "Epoch 79/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 1.0097 - accuracy: 0.7611 - val_loss: 0.9894 - val_accuracy: 0.7772\n",
      "Epoch 80/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.9828 - accuracy: 0.7709 - val_loss: 1.0265 - val_accuracy: 0.7398\n",
      "Epoch 81/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 1.0156 - accuracy: 0.7568 - val_loss: 1.0202 - val_accuracy: 0.7721\n",
      "Epoch 82/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 1.0073 - accuracy: 0.7641 - val_loss: 1.0409 - val_accuracy: 0.7381\n",
      "Epoch 83/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.9932 - accuracy: 0.7666 - val_loss: 0.9380 - val_accuracy: 0.8027\n",
      "Epoch 84/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.9680 - accuracy: 0.7717 - val_loss: 0.9856 - val_accuracy: 0.7636\n",
      "Epoch 85/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.9645 - accuracy: 0.7883 - val_loss: 0.9439 - val_accuracy: 0.7874\n",
      "Epoch 86/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.9626 - accuracy: 0.7794 - val_loss: 0.9683 - val_accuracy: 0.7857\n",
      "Epoch 87/512\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.9553 - accuracy: 0.7896 - val_loss: 0.9131 - val_accuracy: 0.8180\n",
      "Epoch 88/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.9237 - accuracy: 0.8007 - val_loss: 0.9361 - val_accuracy: 0.7959\n",
      "Epoch 89/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.9221 - accuracy: 0.7917 - val_loss: 0.8882 - val_accuracy: 0.8163\n",
      "Epoch 90/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.9262 - accuracy: 0.7913 - val_loss: 0.9516 - val_accuracy: 0.7755\n",
      "Epoch 91/512\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.9120 - accuracy: 0.7947 - val_loss: 0.8814 - val_accuracy: 0.8248\n",
      "Epoch 92/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.9091 - accuracy: 0.8037 - val_loss: 0.9539 - val_accuracy: 0.7602\n",
      "Epoch 93/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.9033 - accuracy: 0.7994 - val_loss: 0.8767 - val_accuracy: 0.8231\n",
      "Epoch 94/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.8964 - accuracy: 0.8083 - val_loss: 0.9145 - val_accuracy: 0.7874\n",
      "Epoch 95/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.8912 - accuracy: 0.8049 - val_loss: 0.8686 - val_accuracy: 0.8146\n",
      "Epoch 96/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.9038 - accuracy: 0.7939 - val_loss: 0.9024 - val_accuracy: 0.8044\n",
      "Epoch 97/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.8828 - accuracy: 0.8130 - val_loss: 0.8506 - val_accuracy: 0.8316\n",
      "Epoch 98/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.8533 - accuracy: 0.8177 - val_loss: 0.8677 - val_accuracy: 0.8265\n",
      "Epoch 99/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.8595 - accuracy: 0.8254 - val_loss: 0.8637 - val_accuracy: 0.8044\n",
      "Epoch 100/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.8672 - accuracy: 0.8126 - val_loss: 0.8471 - val_accuracy: 0.8163\n",
      "Epoch 101/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.8483 - accuracy: 0.8109 - val_loss: 0.8407 - val_accuracy: 0.8384\n",
      "Epoch 102/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.8424 - accuracy: 0.8152 - val_loss: 0.8576 - val_accuracy: 0.8095\n",
      "Epoch 103/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.8531 - accuracy: 0.8203 - val_loss: 0.8375 - val_accuracy: 0.8503\n",
      "Epoch 104/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.8485 - accuracy: 0.8198 - val_loss: 0.8535 - val_accuracy: 0.8112\n",
      "Epoch 105/512\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.8465 - accuracy: 0.8152 - val_loss: 0.8261 - val_accuracy: 0.8401\n",
      "Epoch 106/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.8398 - accuracy: 0.8216 - val_loss: 0.8337 - val_accuracy: 0.8367\n",
      "Epoch 107/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.8223 - accuracy: 0.8313 - val_loss: 0.8351 - val_accuracy: 0.8316\n",
      "Epoch 108/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.8394 - accuracy: 0.8147 - val_loss: 0.8135 - val_accuracy: 0.8503\n",
      "Epoch 109/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.8435 - accuracy: 0.8241 - val_loss: 0.8414 - val_accuracy: 0.8197\n",
      "Epoch 110/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.8385 - accuracy: 0.8207 - val_loss: 0.8238 - val_accuracy: 0.8452\n",
      "Epoch 111/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.8539 - accuracy: 0.8041 - val_loss: 0.8656 - val_accuracy: 0.8112\n",
      "Epoch 112/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.8319 - accuracy: 0.8220 - val_loss: 0.8132 - val_accuracy: 0.8588\n",
      "Epoch 113/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.8188 - accuracy: 0.8267 - val_loss: 0.8324 - val_accuracy: 0.8282\n",
      "Epoch 114/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.8014 - accuracy: 0.8411 - val_loss: 0.7929 - val_accuracy: 0.8520\n",
      "Epoch 115/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.8021 - accuracy: 0.8309 - val_loss: 0.8097 - val_accuracy: 0.8384\n",
      "Epoch 116/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.7808 - accuracy: 0.8441 - val_loss: 0.7735 - val_accuracy: 0.8724\n",
      "Epoch 117/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.7866 - accuracy: 0.8463 - val_loss: 0.7828 - val_accuracy: 0.8486\n",
      "Epoch 118/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.7799 - accuracy: 0.8450 - val_loss: 0.7878 - val_accuracy: 0.8554\n",
      "Epoch 119/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.7679 - accuracy: 0.8492 - val_loss: 0.7809 - val_accuracy: 0.8605\n",
      "Epoch 120/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.7768 - accuracy: 0.8480 - val_loss: 0.7854 - val_accuracy: 0.8622\n",
      "Epoch 121/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.7736 - accuracy: 0.8509 - val_loss: 0.8066 - val_accuracy: 0.8452\n",
      "Epoch 122/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.7704 - accuracy: 0.8411 - val_loss: 0.7769 - val_accuracy: 0.8469\n",
      "Epoch 123/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.7646 - accuracy: 0.8424 - val_loss: 0.8219 - val_accuracy: 0.8435\n",
      "Epoch 124/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.7703 - accuracy: 0.8369 - val_loss: 0.7531 - val_accuracy: 0.8690\n",
      "Epoch 125/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.7416 - accuracy: 0.8548 - val_loss: 0.7673 - val_accuracy: 0.8503\n",
      "Epoch 126/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.7474 - accuracy: 0.8543 - val_loss: 0.7508 - val_accuracy: 0.8537\n",
      "Epoch 127/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.7373 - accuracy: 0.8663 - val_loss: 0.7682 - val_accuracy: 0.8469\n",
      "Epoch 128/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.7412 - accuracy: 0.8556 - val_loss: 0.7443 - val_accuracy: 0.8741\n",
      "Epoch 129/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.7327 - accuracy: 0.8569 - val_loss: 0.8251 - val_accuracy: 0.8299\n",
      "Epoch 130/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.7556 - accuracy: 0.8488 - val_loss: 0.7495 - val_accuracy: 0.8656\n",
      "Epoch 131/512\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.7376 - accuracy: 0.8582 - val_loss: 0.7708 - val_accuracy: 0.8571\n",
      "Epoch 132/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.7278 - accuracy: 0.8573 - val_loss: 0.7425 - val_accuracy: 0.8673\n",
      "Epoch 133/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.7296 - accuracy: 0.8658 - val_loss: 0.7394 - val_accuracy: 0.8605\n",
      "Epoch 134/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.7342 - accuracy: 0.8488 - val_loss: 0.7656 - val_accuracy: 0.8537\n",
      "Epoch 135/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.7285 - accuracy: 0.8522 - val_loss: 0.7552 - val_accuracy: 0.8588\n",
      "Epoch 136/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.7461 - accuracy: 0.8539 - val_loss: 0.7665 - val_accuracy: 0.8435\n",
      "Epoch 137/512\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.7232 - accuracy: 0.8629 - val_loss: 0.7532 - val_accuracy: 0.8605\n",
      "Epoch 138/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.7328 - accuracy: 0.8616 - val_loss: 0.7489 - val_accuracy: 0.8605\n",
      "Epoch 139/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.7124 - accuracy: 0.8607 - val_loss: 0.7245 - val_accuracy: 0.8690\n",
      "Epoch 140/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.7054 - accuracy: 0.8612 - val_loss: 0.7755 - val_accuracy: 0.8384\n",
      "Epoch 141/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.7103 - accuracy: 0.8646 - val_loss: 0.7386 - val_accuracy: 0.8776\n",
      "Epoch 142/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6973 - accuracy: 0.8701 - val_loss: 0.7559 - val_accuracy: 0.8486\n",
      "Epoch 143/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.7003 - accuracy: 0.8607 - val_loss: 0.8065 - val_accuracy: 0.8367\n",
      "Epoch 144/512\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.7285 - accuracy: 0.8450 - val_loss: 0.7690 - val_accuracy: 0.8418\n",
      "Epoch 145/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.7180 - accuracy: 0.8552 - val_loss: 0.7229 - val_accuracy: 0.8929\n",
      "Epoch 146/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6922 - accuracy: 0.8722 - val_loss: 0.7438 - val_accuracy: 0.8469\n",
      "Epoch 147/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6998 - accuracy: 0.8710 - val_loss: 0.7273 - val_accuracy: 0.8690\n",
      "Epoch 148/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6911 - accuracy: 0.8705 - val_loss: 0.7453 - val_accuracy: 0.8554\n",
      "Epoch 149/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6934 - accuracy: 0.8705 - val_loss: 0.7181 - val_accuracy: 0.8793\n",
      "Epoch 150/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6747 - accuracy: 0.8778 - val_loss: 0.7270 - val_accuracy: 0.8639\n",
      "Epoch 151/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6854 - accuracy: 0.8646 - val_loss: 0.7103 - val_accuracy: 0.8690\n",
      "Epoch 152/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6862 - accuracy: 0.8629 - val_loss: 0.7147 - val_accuracy: 0.8707\n",
      "Epoch 153/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6972 - accuracy: 0.8569 - val_loss: 0.7559 - val_accuracy: 0.8486\n",
      "Epoch 154/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6763 - accuracy: 0.8710 - val_loss: 0.7176 - val_accuracy: 0.8588\n",
      "Epoch 155/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6731 - accuracy: 0.8718 - val_loss: 0.7103 - val_accuracy: 0.8759\n",
      "Epoch 156/512\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6663 - accuracy: 0.8863 - val_loss: 0.6924 - val_accuracy: 0.8861\n",
      "Epoch 157/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6550 - accuracy: 0.8820 - val_loss: 0.7155 - val_accuracy: 0.8588\n",
      "Epoch 158/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6557 - accuracy: 0.8782 - val_loss: 0.7168 - val_accuracy: 0.8605\n",
      "Epoch 159/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6729 - accuracy: 0.8765 - val_loss: 0.7258 - val_accuracy: 0.8571\n",
      "Epoch 160/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6647 - accuracy: 0.8748 - val_loss: 0.6954 - val_accuracy: 0.8759\n",
      "Epoch 161/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6652 - accuracy: 0.8748 - val_loss: 0.7211 - val_accuracy: 0.8741\n",
      "Epoch 162/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6618 - accuracy: 0.8829 - val_loss: 0.7391 - val_accuracy: 0.8588\n",
      "Epoch 163/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6835 - accuracy: 0.8573 - val_loss: 0.7661 - val_accuracy: 0.8571\n",
      "Epoch 164/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6920 - accuracy: 0.8684 - val_loss: 0.7627 - val_accuracy: 0.8486\n",
      "Epoch 165/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6823 - accuracy: 0.8688 - val_loss: 0.7361 - val_accuracy: 0.8707\n",
      "Epoch 166/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6652 - accuracy: 0.8714 - val_loss: 0.7349 - val_accuracy: 0.8503\n",
      "Epoch 167/512\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.6504 - accuracy: 0.8799 - val_loss: 0.6957 - val_accuracy: 0.8861\n",
      "Epoch 168/512\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6615 - accuracy: 0.8752 - val_loss: 0.6851 - val_accuracy: 0.8827\n",
      "Epoch 169/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6454 - accuracy: 0.8752 - val_loss: 0.7154 - val_accuracy: 0.8776\n",
      "Epoch 170/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6470 - accuracy: 0.8833 - val_loss: 0.6986 - val_accuracy: 0.8639\n",
      "Epoch 171/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6504 - accuracy: 0.8714 - val_loss: 0.7275 - val_accuracy: 0.8793\n",
      "Epoch 172/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6329 - accuracy: 0.8935 - val_loss: 0.6821 - val_accuracy: 0.8741\n",
      "Epoch 173/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6497 - accuracy: 0.8769 - val_loss: 0.6651 - val_accuracy: 0.8946\n",
      "Epoch 174/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6287 - accuracy: 0.8905 - val_loss: 0.6849 - val_accuracy: 0.8776\n",
      "Epoch 175/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6217 - accuracy: 0.8880 - val_loss: 0.6968 - val_accuracy: 0.8776\n",
      "Epoch 176/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6269 - accuracy: 0.8867 - val_loss: 0.6778 - val_accuracy: 0.8759\n",
      "Epoch 177/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6306 - accuracy: 0.8799 - val_loss: 0.6976 - val_accuracy: 0.8810\n",
      "Epoch 178/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6237 - accuracy: 0.8910 - val_loss: 0.6912 - val_accuracy: 0.8793\n",
      "Epoch 179/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6304 - accuracy: 0.8765 - val_loss: 0.6739 - val_accuracy: 0.8878\n",
      "Epoch 180/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6081 - accuracy: 0.9016 - val_loss: 0.6898 - val_accuracy: 0.8656\n",
      "Epoch 181/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6049 - accuracy: 0.8905 - val_loss: 0.6751 - val_accuracy: 0.8895\n",
      "Epoch 182/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6051 - accuracy: 0.8914 - val_loss: 0.6902 - val_accuracy: 0.8759\n",
      "Epoch 183/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6119 - accuracy: 0.8880 - val_loss: 0.6592 - val_accuracy: 0.8895\n",
      "Epoch 184/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6083 - accuracy: 0.8842 - val_loss: 0.7072 - val_accuracy: 0.8639\n",
      "Epoch 185/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6128 - accuracy: 0.8931 - val_loss: 0.6669 - val_accuracy: 0.8861\n",
      "Epoch 186/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6124 - accuracy: 0.8880 - val_loss: 0.6910 - val_accuracy: 0.8741\n",
      "Epoch 187/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6048 - accuracy: 0.8914 - val_loss: 0.6507 - val_accuracy: 0.8861\n",
      "Epoch 188/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.5843 - accuracy: 0.8940 - val_loss: 0.6874 - val_accuracy: 0.8741\n",
      "Epoch 189/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.5890 - accuracy: 0.8927 - val_loss: 0.6583 - val_accuracy: 0.8810\n",
      "Epoch 190/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5860 - accuracy: 0.9008 - val_loss: 0.6548 - val_accuracy: 0.8946\n",
      "Epoch 191/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.5865 - accuracy: 0.8986 - val_loss: 0.6871 - val_accuracy: 0.8793\n",
      "Epoch 192/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5884 - accuracy: 0.8969 - val_loss: 0.6562 - val_accuracy: 0.8929\n",
      "Epoch 193/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5892 - accuracy: 0.8965 - val_loss: 0.6568 - val_accuracy: 0.8980\n",
      "Epoch 194/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5836 - accuracy: 0.8969 - val_loss: 0.6813 - val_accuracy: 0.8759\n",
      "Epoch 195/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5843 - accuracy: 0.8982 - val_loss: 0.6643 - val_accuracy: 0.8810\n",
      "Epoch 196/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6092 - accuracy: 0.8871 - val_loss: 0.6619 - val_accuracy: 0.8827\n",
      "Epoch 197/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6069 - accuracy: 0.8752 - val_loss: 0.6829 - val_accuracy: 0.8912\n",
      "Epoch 198/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6038 - accuracy: 0.8884 - val_loss: 0.6942 - val_accuracy: 0.8656\n",
      "Epoch 199/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5893 - accuracy: 0.8893 - val_loss: 0.6676 - val_accuracy: 0.8827\n",
      "Epoch 200/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5912 - accuracy: 0.8876 - val_loss: 0.6494 - val_accuracy: 0.8929\n",
      "Epoch 201/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.5859 - accuracy: 0.8927 - val_loss: 0.6612 - val_accuracy: 0.8793\n",
      "Epoch 202/512\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.5885 - accuracy: 0.8965 - val_loss: 0.6673 - val_accuracy: 0.8810\n",
      "Epoch 203/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5738 - accuracy: 0.8922 - val_loss: 0.6980 - val_accuracy: 0.8707\n",
      "Epoch 204/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.5779 - accuracy: 0.8974 - val_loss: 0.6622 - val_accuracy: 0.8827\n",
      "Epoch 205/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5723 - accuracy: 0.8978 - val_loss: 0.6578 - val_accuracy: 0.8759\n",
      "Epoch 206/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.5859 - accuracy: 0.8888 - val_loss: 0.6638 - val_accuracy: 0.8929\n",
      "Epoch 207/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.5828 - accuracy: 0.8957 - val_loss: 0.6483 - val_accuracy: 0.8827\n",
      "Epoch 208/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.5605 - accuracy: 0.9080 - val_loss: 0.6480 - val_accuracy: 0.8946\n",
      "Epoch 209/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5626 - accuracy: 0.9037 - val_loss: 0.6795 - val_accuracy: 0.8759\n",
      "Epoch 210/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5639 - accuracy: 0.8978 - val_loss: 0.6451 - val_accuracy: 0.8912\n",
      "Epoch 211/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.5652 - accuracy: 0.8999 - val_loss: 0.6686 - val_accuracy: 0.8810\n",
      "Epoch 212/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5500 - accuracy: 0.9089 - val_loss: 0.6382 - val_accuracy: 0.9031\n",
      "Epoch 213/512\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.5681 - accuracy: 0.9029 - val_loss: 0.6708 - val_accuracy: 0.8793\n",
      "Epoch 214/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5637 - accuracy: 0.8944 - val_loss: 0.6526 - val_accuracy: 0.8980\n",
      "Epoch 215/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.5663 - accuracy: 0.8986 - val_loss: 0.6685 - val_accuracy: 0.8707\n",
      "Epoch 216/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5656 - accuracy: 0.8927 - val_loss: 0.6405 - val_accuracy: 0.8963\n",
      "Epoch 217/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5638 - accuracy: 0.8940 - val_loss: 0.6658 - val_accuracy: 0.8810\n",
      "Epoch 218/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5651 - accuracy: 0.8957 - val_loss: 0.6263 - val_accuracy: 0.8929\n",
      "Epoch 219/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5502 - accuracy: 0.9059 - val_loss: 0.6337 - val_accuracy: 0.9048\n",
      "Epoch 220/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.5458 - accuracy: 0.9055 - val_loss: 0.6176 - val_accuracy: 0.8912\n",
      "Epoch 221/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5439 - accuracy: 0.9152 - val_loss: 0.6362 - val_accuracy: 0.9082\n",
      "Epoch 222/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.5326 - accuracy: 0.9097 - val_loss: 0.6415 - val_accuracy: 0.8912\n",
      "Epoch 223/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5219 - accuracy: 0.9174 - val_loss: 0.6437 - val_accuracy: 0.8997\n",
      "Epoch 224/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.5369 - accuracy: 0.9037 - val_loss: 0.6205 - val_accuracy: 0.8895\n",
      "Epoch 225/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.5382 - accuracy: 0.9025 - val_loss: 0.6324 - val_accuracy: 0.8946\n",
      "Epoch 226/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5408 - accuracy: 0.9050 - val_loss: 0.6179 - val_accuracy: 0.8980\n",
      "Epoch 227/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.5317 - accuracy: 0.9072 - val_loss: 0.6256 - val_accuracy: 0.8878\n",
      "Epoch 228/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.5345 - accuracy: 0.9067 - val_loss: 0.6077 - val_accuracy: 0.8997\n",
      "Epoch 229/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.5196 - accuracy: 0.9170 - val_loss: 0.6134 - val_accuracy: 0.9065\n",
      "Epoch 230/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5171 - accuracy: 0.9076 - val_loss: 0.6113 - val_accuracy: 0.9031\n",
      "Epoch 231/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.5146 - accuracy: 0.9135 - val_loss: 0.6088 - val_accuracy: 0.9031\n",
      "Epoch 232/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5111 - accuracy: 0.9199 - val_loss: 0.6291 - val_accuracy: 0.8980\n",
      "Epoch 233/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.5366 - accuracy: 0.9089 - val_loss: 0.6268 - val_accuracy: 0.9031\n",
      "Epoch 234/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.5190 - accuracy: 0.9144 - val_loss: 0.6118 - val_accuracy: 0.9014\n",
      "Epoch 235/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5215 - accuracy: 0.9029 - val_loss: 0.6346 - val_accuracy: 0.8963\n",
      "Epoch 236/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.5207 - accuracy: 0.9123 - val_loss: 0.6291 - val_accuracy: 0.8929\n",
      "Epoch 237/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5295 - accuracy: 0.9093 - val_loss: 0.6198 - val_accuracy: 0.9031\n",
      "Epoch 238/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5195 - accuracy: 0.9195 - val_loss: 0.6132 - val_accuracy: 0.9048\n",
      "Epoch 239/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5356 - accuracy: 0.9042 - val_loss: 0.6300 - val_accuracy: 0.8861\n",
      "Epoch 240/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5304 - accuracy: 0.9067 - val_loss: 0.6044 - val_accuracy: 0.8997\n",
      "Epoch 241/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5207 - accuracy: 0.9067 - val_loss: 0.6355 - val_accuracy: 0.8827\n",
      "Epoch 242/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.5305 - accuracy: 0.9033 - val_loss: 0.6394 - val_accuracy: 0.8878\n",
      "Epoch 243/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5387 - accuracy: 0.8974 - val_loss: 0.6462 - val_accuracy: 0.8759\n",
      "Epoch 244/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.5472 - accuracy: 0.8974 - val_loss: 0.6226 - val_accuracy: 0.8980\n",
      "Epoch 245/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.5269 - accuracy: 0.9029 - val_loss: 0.6387 - val_accuracy: 0.8827\n",
      "Epoch 246/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5314 - accuracy: 0.9012 - val_loss: 0.6269 - val_accuracy: 0.8963\n",
      "Epoch 247/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5275 - accuracy: 0.9050 - val_loss: 0.6076 - val_accuracy: 0.8912\n",
      "Epoch 248/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.5097 - accuracy: 0.9059 - val_loss: 0.6292 - val_accuracy: 0.8793\n",
      "Epoch 249/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.5168 - accuracy: 0.9144 - val_loss: 0.6042 - val_accuracy: 0.8997\n",
      "Epoch 250/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5123 - accuracy: 0.9127 - val_loss: 0.6145 - val_accuracy: 0.8997\n",
      "Epoch 251/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5254 - accuracy: 0.9055 - val_loss: 0.5944 - val_accuracy: 0.9082\n",
      "Epoch 252/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5067 - accuracy: 0.9135 - val_loss: 0.6111 - val_accuracy: 0.8895\n",
      "Epoch 253/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4952 - accuracy: 0.9144 - val_loss: 0.6042 - val_accuracy: 0.8946\n",
      "Epoch 254/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.5091 - accuracy: 0.9059 - val_loss: 0.6213 - val_accuracy: 0.8963\n",
      "Epoch 255/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5114 - accuracy: 0.9123 - val_loss: 0.6045 - val_accuracy: 0.9014\n",
      "Epoch 256/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.5042 - accuracy: 0.9093 - val_loss: 0.6787 - val_accuracy: 0.8520\n",
      "Epoch 257/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.5196 - accuracy: 0.9020 - val_loss: 0.6344 - val_accuracy: 0.8844\n",
      "Epoch 258/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5283 - accuracy: 0.9020 - val_loss: 0.6291 - val_accuracy: 0.8810\n",
      "Epoch 259/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5087 - accuracy: 0.9029 - val_loss: 0.6107 - val_accuracy: 0.9031\n",
      "Epoch 260/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.5129 - accuracy: 0.9050 - val_loss: 0.6348 - val_accuracy: 0.8861\n",
      "Epoch 261/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.5082 - accuracy: 0.9080 - val_loss: 0.5894 - val_accuracy: 0.8980\n",
      "Epoch 262/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5036 - accuracy: 0.9084 - val_loss: 0.5903 - val_accuracy: 0.9048\n",
      "Epoch 263/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4858 - accuracy: 0.9204 - val_loss: 0.5941 - val_accuracy: 0.8946\n",
      "Epoch 264/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4817 - accuracy: 0.9170 - val_loss: 0.6403 - val_accuracy: 0.8759\n",
      "Epoch 265/512\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4902 - accuracy: 0.9161 - val_loss: 0.5747 - val_accuracy: 0.9201\n",
      "Epoch 266/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4724 - accuracy: 0.9233 - val_loss: 0.6029 - val_accuracy: 0.8980\n",
      "Epoch 267/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4741 - accuracy: 0.9199 - val_loss: 0.6068 - val_accuracy: 0.8929\n",
      "Epoch 268/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4915 - accuracy: 0.9195 - val_loss: 0.5833 - val_accuracy: 0.9014\n",
      "Epoch 269/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4785 - accuracy: 0.9170 - val_loss: 0.5981 - val_accuracy: 0.8963\n",
      "Epoch 270/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4702 - accuracy: 0.9331 - val_loss: 0.5783 - val_accuracy: 0.9082\n",
      "Epoch 271/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4709 - accuracy: 0.9161 - val_loss: 0.5990 - val_accuracy: 0.8946\n",
      "Epoch 272/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4647 - accuracy: 0.9199 - val_loss: 0.5892 - val_accuracy: 0.9065\n",
      "Epoch 273/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4796 - accuracy: 0.9187 - val_loss: 0.6094 - val_accuracy: 0.8929\n",
      "Epoch 274/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4842 - accuracy: 0.9114 - val_loss: 0.5741 - val_accuracy: 0.9014\n",
      "Epoch 275/512\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4709 - accuracy: 0.9250 - val_loss: 0.5987 - val_accuracy: 0.8980\n",
      "Epoch 276/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4794 - accuracy: 0.9170 - val_loss: 0.6762 - val_accuracy: 0.8810\n",
      "Epoch 277/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.5106 - accuracy: 0.9084 - val_loss: 0.5897 - val_accuracy: 0.8980\n",
      "Epoch 278/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4867 - accuracy: 0.9182 - val_loss: 0.5916 - val_accuracy: 0.8997\n",
      "Epoch 279/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5012 - accuracy: 0.9161 - val_loss: 0.5905 - val_accuracy: 0.8980\n",
      "Epoch 280/512\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4876 - accuracy: 0.9097 - val_loss: 0.5864 - val_accuracy: 0.9116\n",
      "Epoch 281/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4808 - accuracy: 0.9170 - val_loss: 0.5703 - val_accuracy: 0.9048\n",
      "Epoch 282/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4597 - accuracy: 0.9233 - val_loss: 0.5650 - val_accuracy: 0.8980\n",
      "Epoch 283/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4540 - accuracy: 0.9331 - val_loss: 0.5612 - val_accuracy: 0.9031\n",
      "Epoch 284/512\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4541 - accuracy: 0.9310 - val_loss: 0.5544 - val_accuracy: 0.9065\n",
      "Epoch 285/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4488 - accuracy: 0.9297 - val_loss: 0.5664 - val_accuracy: 0.9048\n",
      "Epoch 286/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4427 - accuracy: 0.9331 - val_loss: 0.5781 - val_accuracy: 0.9133\n",
      "Epoch 287/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4547 - accuracy: 0.9212 - val_loss: 0.5811 - val_accuracy: 0.9014\n",
      "Epoch 288/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4347 - accuracy: 0.9395 - val_loss: 0.5651 - val_accuracy: 0.9082\n",
      "Epoch 289/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4529 - accuracy: 0.9259 - val_loss: 0.5700 - val_accuracy: 0.9065\n",
      "Epoch 290/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4418 - accuracy: 0.9319 - val_loss: 0.5994 - val_accuracy: 0.8929\n",
      "Epoch 291/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4478 - accuracy: 0.9238 - val_loss: 0.5747 - val_accuracy: 0.9133\n",
      "Epoch 292/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4690 - accuracy: 0.9157 - val_loss: 0.5736 - val_accuracy: 0.9048\n",
      "Epoch 293/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4571 - accuracy: 0.9195 - val_loss: 0.5581 - val_accuracy: 0.9201\n",
      "Epoch 294/512\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4446 - accuracy: 0.9323 - val_loss: 0.5743 - val_accuracy: 0.9133\n",
      "Epoch 295/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4560 - accuracy: 0.9208 - val_loss: 0.5490 - val_accuracy: 0.9133\n",
      "Epoch 296/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4353 - accuracy: 0.9314 - val_loss: 0.5772 - val_accuracy: 0.9031\n",
      "Epoch 297/512\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4428 - accuracy: 0.9233 - val_loss: 0.5580 - val_accuracy: 0.9167\n",
      "Epoch 298/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4438 - accuracy: 0.9238 - val_loss: 0.5704 - val_accuracy: 0.9031\n",
      "Epoch 299/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4420 - accuracy: 0.9233 - val_loss: 0.5886 - val_accuracy: 0.8997\n",
      "Epoch 300/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4603 - accuracy: 0.9250 - val_loss: 0.5528 - val_accuracy: 0.9099\n",
      "Epoch 301/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4460 - accuracy: 0.9280 - val_loss: 0.5870 - val_accuracy: 0.8929\n",
      "Epoch 302/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4527 - accuracy: 0.9191 - val_loss: 0.5798 - val_accuracy: 0.8980\n",
      "Epoch 303/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4417 - accuracy: 0.9306 - val_loss: 0.5958 - val_accuracy: 0.8929\n",
      "Epoch 304/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4500 - accuracy: 0.9238 - val_loss: 0.5947 - val_accuracy: 0.9031\n",
      "Epoch 305/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4379 - accuracy: 0.9284 - val_loss: 0.5747 - val_accuracy: 0.9031\n",
      "Epoch 306/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4515 - accuracy: 0.9204 - val_loss: 0.5680 - val_accuracy: 0.9167\n",
      "Epoch 307/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4532 - accuracy: 0.9195 - val_loss: 0.5913 - val_accuracy: 0.8963\n",
      "Epoch 308/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4469 - accuracy: 0.9221 - val_loss: 0.6077 - val_accuracy: 0.8963\n",
      "Epoch 309/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4543 - accuracy: 0.9233 - val_loss: 0.5694 - val_accuracy: 0.9082\n",
      "Epoch 310/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4365 - accuracy: 0.9250 - val_loss: 0.5622 - val_accuracy: 0.8963\n",
      "Epoch 311/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4219 - accuracy: 0.9374 - val_loss: 0.5509 - val_accuracy: 0.9116\n",
      "Epoch 312/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4200 - accuracy: 0.9353 - val_loss: 0.5569 - val_accuracy: 0.9133\n",
      "Epoch 313/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4264 - accuracy: 0.9344 - val_loss: 0.5712 - val_accuracy: 0.9065\n",
      "Epoch 314/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4343 - accuracy: 0.9306 - val_loss: 0.5909 - val_accuracy: 0.8963\n",
      "Epoch 315/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4304 - accuracy: 0.9306 - val_loss: 0.5767 - val_accuracy: 0.9269\n",
      "Epoch 316/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4143 - accuracy: 0.9399 - val_loss: 0.5594 - val_accuracy: 0.9116\n",
      "Epoch 317/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4132 - accuracy: 0.9395 - val_loss: 0.5767 - val_accuracy: 0.9082\n",
      "Epoch 318/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4238 - accuracy: 0.9353 - val_loss: 0.5800 - val_accuracy: 0.8844\n",
      "Epoch 319/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4194 - accuracy: 0.9297 - val_loss: 0.5413 - val_accuracy: 0.9150\n",
      "Epoch 320/512\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4103 - accuracy: 0.9408 - val_loss: 0.5511 - val_accuracy: 0.9116\n",
      "Epoch 321/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4056 - accuracy: 0.9395 - val_loss: 0.5670 - val_accuracy: 0.9150\n",
      "Epoch 322/512\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4184 - accuracy: 0.9361 - val_loss: 0.5760 - val_accuracy: 0.8980\n",
      "Epoch 323/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4185 - accuracy: 0.9327 - val_loss: 0.5729 - val_accuracy: 0.9031\n",
      "Epoch 324/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4172 - accuracy: 0.9365 - val_loss: 0.5611 - val_accuracy: 0.9184\n",
      "Epoch 325/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4244 - accuracy: 0.9297 - val_loss: 0.5868 - val_accuracy: 0.8963\n",
      "Epoch 326/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4248 - accuracy: 0.9319 - val_loss: 0.5846 - val_accuracy: 0.8861\n",
      "Epoch 327/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4209 - accuracy: 0.9267 - val_loss: 0.5496 - val_accuracy: 0.9150\n",
      "Epoch 328/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4174 - accuracy: 0.9314 - val_loss: 0.6044 - val_accuracy: 0.8980\n",
      "Epoch 329/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4284 - accuracy: 0.9263 - val_loss: 0.5838 - val_accuracy: 0.9082\n",
      "Epoch 330/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4173 - accuracy: 0.9310 - val_loss: 0.5520 - val_accuracy: 0.9065\n",
      "Epoch 331/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4288 - accuracy: 0.9319 - val_loss: 0.5421 - val_accuracy: 0.9184\n",
      "Epoch 332/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4256 - accuracy: 0.9310 - val_loss: 0.5921 - val_accuracy: 0.8980\n",
      "Epoch 333/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4215 - accuracy: 0.9250 - val_loss: 0.5718 - val_accuracy: 0.8946\n",
      "Epoch 334/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4131 - accuracy: 0.9340 - val_loss: 0.5567 - val_accuracy: 0.9286\n",
      "Epoch 335/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4139 - accuracy: 0.9348 - val_loss: 0.5762 - val_accuracy: 0.9116\n",
      "Epoch 336/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4029 - accuracy: 0.9374 - val_loss: 0.5874 - val_accuracy: 0.8929\n",
      "Epoch 337/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4259 - accuracy: 0.9289 - val_loss: 0.5724 - val_accuracy: 0.9065\n",
      "Epoch 338/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4286 - accuracy: 0.9212 - val_loss: 0.5831 - val_accuracy: 0.8810\n",
      "Epoch 339/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4160 - accuracy: 0.9267 - val_loss: 0.5822 - val_accuracy: 0.9065\n",
      "Epoch 340/512\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4293 - accuracy: 0.9246 - val_loss: 0.5798 - val_accuracy: 0.8878\n",
      "Epoch 341/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4043 - accuracy: 0.9378 - val_loss: 0.5593 - val_accuracy: 0.9167\n",
      "Epoch 342/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4113 - accuracy: 0.9336 - val_loss: 0.5630 - val_accuracy: 0.9031\n",
      "Epoch 343/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.3990 - accuracy: 0.9387 - val_loss: 0.5330 - val_accuracy: 0.9133\n",
      "Epoch 344/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4027 - accuracy: 0.9378 - val_loss: 0.5791 - val_accuracy: 0.8997\n",
      "Epoch 345/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4018 - accuracy: 0.9353 - val_loss: 0.5691 - val_accuracy: 0.9048\n",
      "Epoch 346/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4036 - accuracy: 0.9404 - val_loss: 0.5935 - val_accuracy: 0.9014\n",
      "Epoch 347/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4075 - accuracy: 0.9382 - val_loss: 0.5648 - val_accuracy: 0.9031\n",
      "Epoch 348/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4048 - accuracy: 0.9421 - val_loss: 0.5976 - val_accuracy: 0.8912\n",
      "Epoch 349/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4010 - accuracy: 0.9374 - val_loss: 0.5345 - val_accuracy: 0.9167\n",
      "Epoch 350/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3891 - accuracy: 0.9387 - val_loss: 0.5615 - val_accuracy: 0.9099\n",
      "Epoch 351/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4058 - accuracy: 0.9327 - val_loss: 0.5625 - val_accuracy: 0.9201\n",
      "Epoch 352/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3910 - accuracy: 0.9382 - val_loss: 0.5557 - val_accuracy: 0.9116\n",
      "Epoch 353/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3978 - accuracy: 0.9365 - val_loss: 0.5589 - val_accuracy: 0.9031\n",
      "Epoch 354/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3911 - accuracy: 0.9421 - val_loss: 0.5593 - val_accuracy: 0.9099\n",
      "Epoch 355/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3996 - accuracy: 0.9344 - val_loss: 0.5673 - val_accuracy: 0.9167\n",
      "Epoch 356/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3970 - accuracy: 0.9331 - val_loss: 0.5334 - val_accuracy: 0.9150\n",
      "Epoch 357/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3879 - accuracy: 0.9361 - val_loss: 0.5478 - val_accuracy: 0.9031\n",
      "Epoch 358/512\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.3983 - accuracy: 0.9353 - val_loss: 0.5812 - val_accuracy: 0.8929\n",
      "Epoch 359/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4125 - accuracy: 0.9365 - val_loss: 0.5883 - val_accuracy: 0.8963\n",
      "Epoch 360/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4083 - accuracy: 0.9374 - val_loss: 0.5611 - val_accuracy: 0.8997\n",
      "Epoch 361/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4080 - accuracy: 0.9246 - val_loss: 0.5792 - val_accuracy: 0.8980\n",
      "Epoch 362/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4344 - accuracy: 0.9212 - val_loss: 0.5620 - val_accuracy: 0.9065\n",
      "Epoch 363/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4089 - accuracy: 0.9297 - val_loss: 0.5555 - val_accuracy: 0.9082\n",
      "Epoch 364/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4007 - accuracy: 0.9340 - val_loss: 0.5645 - val_accuracy: 0.9167\n",
      "Epoch 365/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3877 - accuracy: 0.9425 - val_loss: 0.5390 - val_accuracy: 0.9116\n",
      "Epoch 366/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3887 - accuracy: 0.9412 - val_loss: 0.5435 - val_accuracy: 0.9235\n",
      "Epoch 367/512\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.3871 - accuracy: 0.9353 - val_loss: 0.5524 - val_accuracy: 0.9082\n",
      "Epoch 368/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3798 - accuracy: 0.9412 - val_loss: 0.5618 - val_accuracy: 0.9184\n",
      "Epoch 369/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3846 - accuracy: 0.9412 - val_loss: 0.5481 - val_accuracy: 0.9014\n",
      "Epoch 370/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3882 - accuracy: 0.9399 - val_loss: 0.5491 - val_accuracy: 0.9184\n",
      "Epoch 371/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3855 - accuracy: 0.9344 - val_loss: 0.5378 - val_accuracy: 0.9201\n",
      "Epoch 372/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3764 - accuracy: 0.9459 - val_loss: 0.5783 - val_accuracy: 0.8963\n",
      "Epoch 373/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3884 - accuracy: 0.9395 - val_loss: 0.5407 - val_accuracy: 0.9201\n",
      "Epoch 374/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3984 - accuracy: 0.9353 - val_loss: 0.5381 - val_accuracy: 0.9133\n",
      "Epoch 375/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.3560 - accuracy: 0.9561 - val_loss: 0.5562 - val_accuracy: 0.9082\n",
      "Epoch 376/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3872 - accuracy: 0.9306 - val_loss: 0.5477 - val_accuracy: 0.9235\n",
      "Epoch 377/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3691 - accuracy: 0.9472 - val_loss: 0.5591 - val_accuracy: 0.9099\n",
      "Epoch 378/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.3769 - accuracy: 0.9399 - val_loss: 0.5266 - val_accuracy: 0.9167\n",
      "Epoch 379/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3650 - accuracy: 0.9485 - val_loss: 0.5478 - val_accuracy: 0.9116\n",
      "Epoch 380/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.3632 - accuracy: 0.9506 - val_loss: 0.5145 - val_accuracy: 0.9286\n",
      "Epoch 381/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3606 - accuracy: 0.9480 - val_loss: 0.5334 - val_accuracy: 0.9150\n",
      "Epoch 382/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3541 - accuracy: 0.9451 - val_loss: 0.5369 - val_accuracy: 0.9116\n",
      "Epoch 383/512\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.3687 - accuracy: 0.9455 - val_loss: 0.5273 - val_accuracy: 0.9201\n",
      "Epoch 384/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3691 - accuracy: 0.9446 - val_loss: 0.5245 - val_accuracy: 0.9252\n",
      "Epoch 385/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3809 - accuracy: 0.9365 - val_loss: 0.5404 - val_accuracy: 0.9048\n",
      "Epoch 386/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3607 - accuracy: 0.9472 - val_loss: 0.5403 - val_accuracy: 0.9218\n",
      "Epoch 387/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.3721 - accuracy: 0.9387 - val_loss: 0.5336 - val_accuracy: 0.9167\n",
      "Epoch 388/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3545 - accuracy: 0.9468 - val_loss: 0.5388 - val_accuracy: 0.9133\n",
      "Epoch 389/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3623 - accuracy: 0.9463 - val_loss: 0.5407 - val_accuracy: 0.9133\n",
      "Epoch 390/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3611 - accuracy: 0.9510 - val_loss: 0.5371 - val_accuracy: 0.9133\n",
      "Epoch 391/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3701 - accuracy: 0.9370 - val_loss: 0.5408 - val_accuracy: 0.9150\n",
      "Epoch 392/512\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.3789 - accuracy: 0.9417 - val_loss: 0.5035 - val_accuracy: 0.9337\n",
      "Epoch 393/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.3646 - accuracy: 0.9434 - val_loss: 0.5300 - val_accuracy: 0.9184\n",
      "Epoch 394/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3569 - accuracy: 0.9506 - val_loss: 0.5314 - val_accuracy: 0.9167\n",
      "Epoch 395/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3496 - accuracy: 0.9536 - val_loss: 0.5213 - val_accuracy: 0.9269\n",
      "Epoch 396/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.3554 - accuracy: 0.9480 - val_loss: 0.5595 - val_accuracy: 0.8997\n",
      "Epoch 397/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3564 - accuracy: 0.9472 - val_loss: 0.5555 - val_accuracy: 0.9031\n",
      "Epoch 398/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3748 - accuracy: 0.9378 - val_loss: 0.6226 - val_accuracy: 0.8707\n",
      "Epoch 399/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3784 - accuracy: 0.9404 - val_loss: 0.5286 - val_accuracy: 0.9099\n",
      "Epoch 400/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.3607 - accuracy: 0.9387 - val_loss: 0.5340 - val_accuracy: 0.9320\n",
      "Epoch 401/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3500 - accuracy: 0.9502 - val_loss: 0.5350 - val_accuracy: 0.9218\n",
      "Epoch 402/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3497 - accuracy: 0.9540 - val_loss: 0.5463 - val_accuracy: 0.9099\n",
      "Epoch 403/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3603 - accuracy: 0.9404 - val_loss: 0.5404 - val_accuracy: 0.9269\n",
      "Epoch 404/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3543 - accuracy: 0.9446 - val_loss: 0.5433 - val_accuracy: 0.9150\n",
      "Epoch 405/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3602 - accuracy: 0.9425 - val_loss: 0.5441 - val_accuracy: 0.9286\n",
      "Epoch 406/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3562 - accuracy: 0.9412 - val_loss: 0.5465 - val_accuracy: 0.9150\n",
      "Epoch 407/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3597 - accuracy: 0.9417 - val_loss: 0.5516 - val_accuracy: 0.9082\n",
      "Epoch 408/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.3597 - accuracy: 0.9382 - val_loss: 0.5242 - val_accuracy: 0.9150\n",
      "Epoch 409/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3613 - accuracy: 0.9387 - val_loss: 0.5564 - val_accuracy: 0.9167\n",
      "Epoch 410/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3795 - accuracy: 0.9353 - val_loss: 0.5486 - val_accuracy: 0.8810\n",
      "Epoch 411/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3653 - accuracy: 0.9408 - val_loss: 0.5218 - val_accuracy: 0.9269\n",
      "Epoch 412/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3487 - accuracy: 0.9510 - val_loss: 0.5330 - val_accuracy: 0.9048\n",
      "Epoch 413/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3506 - accuracy: 0.9446 - val_loss: 0.5426 - val_accuracy: 0.9082\n",
      "Epoch 414/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3535 - accuracy: 0.9442 - val_loss: 0.5222 - val_accuracy: 0.9150\n",
      "Epoch 415/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3537 - accuracy: 0.9519 - val_loss: 0.5170 - val_accuracy: 0.9252\n",
      "Epoch 416/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.3512 - accuracy: 0.9451 - val_loss: 0.5461 - val_accuracy: 0.9099\n",
      "Epoch 417/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3483 - accuracy: 0.9455 - val_loss: 0.5277 - val_accuracy: 0.9218\n",
      "Epoch 418/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3473 - accuracy: 0.9451 - val_loss: 0.5267 - val_accuracy: 0.9269\n",
      "Epoch 419/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3566 - accuracy: 0.9425 - val_loss: 0.5233 - val_accuracy: 0.9235\n",
      "Epoch 420/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3299 - accuracy: 0.9583 - val_loss: 0.5288 - val_accuracy: 0.9116\n",
      "Epoch 421/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3373 - accuracy: 0.9497 - val_loss: 0.5202 - val_accuracy: 0.9167\n",
      "Epoch 422/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3389 - accuracy: 0.9532 - val_loss: 0.5532 - val_accuracy: 0.9099\n",
      "Epoch 423/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3319 - accuracy: 0.9566 - val_loss: 0.5208 - val_accuracy: 0.9201\n",
      "Epoch 424/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3296 - accuracy: 0.9587 - val_loss: 0.5187 - val_accuracy: 0.9286\n",
      "Epoch 425/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3339 - accuracy: 0.9536 - val_loss: 0.5235 - val_accuracy: 0.9116\n",
      "Epoch 426/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3354 - accuracy: 0.9523 - val_loss: 0.5397 - val_accuracy: 0.9065\n",
      "Epoch 427/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3288 - accuracy: 0.9540 - val_loss: 0.5254 - val_accuracy: 0.9252\n",
      "Epoch 428/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3266 - accuracy: 0.9600 - val_loss: 0.5326 - val_accuracy: 0.9116\n",
      "Epoch 429/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3435 - accuracy: 0.9472 - val_loss: 0.5224 - val_accuracy: 0.9218\n",
      "Epoch 430/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3350 - accuracy: 0.9502 - val_loss: 0.5271 - val_accuracy: 0.9201\n",
      "Epoch 431/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3576 - accuracy: 0.9365 - val_loss: 0.5442 - val_accuracy: 0.9184\n",
      "Epoch 432/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3450 - accuracy: 0.9429 - val_loss: 0.5259 - val_accuracy: 0.9303\n",
      "Epoch 433/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3362 - accuracy: 0.9527 - val_loss: 0.5374 - val_accuracy: 0.9082\n",
      "Epoch 434/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3278 - accuracy: 0.9523 - val_loss: 0.5327 - val_accuracy: 0.9167\n",
      "Epoch 435/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3389 - accuracy: 0.9489 - val_loss: 0.5238 - val_accuracy: 0.9235\n",
      "Epoch 436/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3393 - accuracy: 0.9519 - val_loss: 0.5237 - val_accuracy: 0.9167\n",
      "Epoch 437/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3361 - accuracy: 0.9472 - val_loss: 0.5273 - val_accuracy: 0.9048\n",
      "Epoch 438/512\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3267 - accuracy: 0.9557 - val_loss: 0.5140 - val_accuracy: 0.9269\n",
      "Epoch 439/512\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3242 - accuracy: 0.9566 - val_loss: 0.5115 - val_accuracy: 0.9269\n",
      "Epoch 440/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.3209 - accuracy: 0.9557 - val_loss: 0.5159 - val_accuracy: 0.9099\n",
      "Epoch 441/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.3262 - accuracy: 0.9557 - val_loss: 0.5459 - val_accuracy: 0.9201\n",
      "Epoch 442/512\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.3382 - accuracy: 0.9476 - val_loss: 0.5525 - val_accuracy: 0.9065\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5035 - accuracy: 0.9337\n",
      "Test accuracy: 0.9336734414100647\n"
     ]
    }
   ],
   "source": [
    "import cvnn.layers as complex_layers\n",
    "import cvnn.losses as complex_losses\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Flatten, Dropout, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras import regularizers\n",
    "\n",
    "def get_model(downSampleRate = 1):\n",
    "    inputs = complex_layers.complex_input(shape=(2000//downSampleRate,1,))\n",
    "    c0 = complex_layers.ComplexConv1D(filters= 2, activation='cart_relu', kernel_size=128//downSampleRate)(inputs)\n",
    "    # c0 = complex_layers.ComplexConv1D(filters= 2, activation='cart_relu', kernel_size=128//downSampleRate)(c0)\n",
    "    c0 = complex_layers.ComplexAvgPooling1D(pool_size=2)(c0)\n",
    "    c1 = complex_layers.ComplexConv1D(filters= 2, activation='cart_relu', kernel_size=128//downSampleRate)(c0)\n",
    "    # c1 = complex_layers.ComplexConv1D(filters= 2, activation='cart_relu', kernel_size=128//downSampleRate)(c1)\n",
    "    c1 = complex_layers.ComplexAvgPooling1D(pool_size=2)(c1)\n",
    "    # c1 = complex_layers.ComplexDropout(0.1/downSampleRate)(c1)\n",
    "    c1 = complex_layers.ComplexFlatten()(c1)\n",
    "    # c1 = complex_layers.ComplexDense(64, activation='cart_relu', kernel_regularizer=regularizers.L1(0.001))(c1)\n",
    "    c1 = complex_layers.ComplexDropout(0.1/downSampleRate)(c1)\n",
    "    out = complex_layers.ComplexDense(100, activation='convert_to_real_with_abs', kernel_regularizer=regularizers.L1(0.0001))(c1)\n",
    "\n",
    "    out = Dense(100, activation='relu')(out)\n",
    "    out = Dense(y_test_encoded.shape[1], activation='softmax')(out)  # 13 classes\n",
    "\n",
    "    return tf.keras.Model(inputs, out)\n",
    "\n",
    "model = get_model()\n",
    "model.summary()\n",
    "# 1/0\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train_encoded, epochs=512, batch_size=1000, validation_data=(X_test, y_test_encoded), callbacks=[early_stopping],verbose=1)\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test_encoded)\n",
    "print(f'Test accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previous methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_old, X_test_old, y_train_old, y_test_old = train_test_split(df['normalized_input_feature_realImage'], df['dvc'], test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_old = tf.convert_to_tensor(X_train_old.tolist())\n",
    "X_test_old =  tf.convert_to_tensor(X_test_old.tolist())\n",
    "y_train_old =  tf.convert_to_tensor(y_train_old.tolist())\n",
    "y_test_old = tf.convert_to_tensor(y_test_old.tolist())\n",
    "\n",
    "data_shape = len(df['data'][0])\n",
    "\n",
    "y_train_encoded_old = to_categorical(y_train_old)\n",
    "y_test_encoded_old = to_categorical(y_test_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2576, 2, 2000])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_old.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-13 10:49:31.867453: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-13 10:49:31.891792: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-13 10:49:31.891819: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-13 10:49:31.892485: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-13 10:49:31.896600: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-13 10:49:32.414477: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "One of the dimensions in the output is <= 0 due to downsampling in conv2d. Consider increasing the input size. Received input shape [None, 2, 2000, 1] which would produce output shape with a zero or negative value in a dimension.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense, Conv2D, Flatten, Dropout, Input\n\u001b[1;32m      5\u001b[0m input_data \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2000\u001b[39m,\u001b[38;5;241m1\u001b[39m,))\n\u001b[0;32m----> 6\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mConv2D\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m x \u001b[38;5;241m=\u001b[39m Conv2D(filters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m,)(x)\n\u001b[1;32m      8\u001b[0m x \u001b[38;5;241m=\u001b[39m Dropout(\u001b[38;5;241m0.5\u001b[39m)(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:354\u001b[0m, in \u001b[0;36mConv.compute_output_shape\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mTensorShape(\n\u001b[1;32m    348\u001b[0m             input_shape[:batch_rank]\n\u001b[1;32m    349\u001b[0m             \u001b[38;5;241m+\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilters]\n\u001b[1;32m    350\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spatial_output_shape(input_shape[batch_rank \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m :])\n\u001b[1;32m    351\u001b[0m         )\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m--> 354\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    355\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOne of the dimensions in the output is <= 0 \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    356\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdue to downsampling in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Consider \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    357\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincreasing the input size. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    358\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived input shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m which would produce \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    359\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput shape with a zero or negative value in a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    360\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdimension.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    361\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: One of the dimensions in the output is <= 0 due to downsampling in conv2d. Consider increasing the input size. Received input shape [None, 2, 2000, 1] which would produce output shape with a zero or negative value in a dimension."
     ]
    }
   ],
   "source": [
    "import cvnn.layers as complex_layers\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, Input\n",
    "\n",
    "input_data = Input(shape=(2,2000,1,))\n",
    "x = Conv2D(filters=5, kernel_size=128, activation='relu',)(input_data)\n",
    "x = Conv2D(filters=5, kernel_size=128, activation='relu',)(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(100, activation='relu',kernel_regularizer=regularizers.L1(0.001))(x)\n",
    "x = Dense(100, activation='relu',kernel_regularizer=regularizers.L1(0.001))(x)\n",
    "output = Dense(y_test_encoded.shape[1], activation='softmax')(x)  # 13 classes\n",
    "\n",
    "\n",
    "model = Model(inputs=input_data, outputs=output)\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_old, y_train_encoded_old, epochs=512, batch_size=64, validation_data=(X_test_old, y_test_encoded_old))\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "loss_old, accuracy_old = model.evaluate(X_test_old, y_test_encoded_old)\n",
    "print(f'Test accuracy: {accuracy_old}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:00<00:00, 12839007.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 108447353.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1648877/1648877 [00:00<00:00, 14338238.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 35876702.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Train Epoch:   0 [     0/ 60000 (  0%)]\tLoss: 2.437958\n",
      "Train Epoch:   0 [  6400/ 60000 ( 11%)]\tLoss: 0.084289\n",
      "Train Epoch:   0 [ 12800/ 60000 ( 21%)]\tLoss: 0.057723\n",
      "Train Epoch:   0 [ 19200/ 60000 ( 32%)]\tLoss: 0.127695\n",
      "Train Epoch:   0 [ 25600/ 60000 ( 43%)]\tLoss: 0.099930\n",
      "Train Epoch:   0 [ 32000/ 60000 ( 53%)]\tLoss: 0.036542\n",
      "Train Epoch:   0 [ 38400/ 60000 ( 64%)]\tLoss: 0.041900\n",
      "Train Epoch:   0 [ 44800/ 60000 ( 75%)]\tLoss: 0.030878\n",
      "Train Epoch:   0 [ 51200/ 60000 ( 85%)]\tLoss: 0.003791\n",
      "Train Epoch:   0 [ 57600/ 60000 ( 96%)]\tLoss: 0.036098\n",
      "Train Epoch:   1 [     0/ 60000 (  0%)]\tLoss: 0.102802\n",
      "Train Epoch:   1 [  6400/ 60000 ( 11%)]\tLoss: 0.013273\n",
      "Train Epoch:   1 [ 12800/ 60000 ( 21%)]\tLoss: 0.004755\n",
      "Train Epoch:   1 [ 19200/ 60000 ( 32%)]\tLoss: 0.068182\n",
      "Train Epoch:   1 [ 25600/ 60000 ( 43%)]\tLoss: 0.052206\n",
      "Train Epoch:   1 [ 32000/ 60000 ( 53%)]\tLoss: 0.081062\n",
      "Train Epoch:   1 [ 38400/ 60000 ( 64%)]\tLoss: 0.014868\n",
      "Train Epoch:   1 [ 44800/ 60000 ( 75%)]\tLoss: 0.049419\n",
      "Train Epoch:   1 [ 51200/ 60000 ( 85%)]\tLoss: 0.002171\n",
      "Train Epoch:   1 [ 57600/ 60000 ( 96%)]\tLoss: 0.015023\n",
      "Train Epoch:   2 [     0/ 60000 (  0%)]\tLoss: 0.001134\n",
      "Train Epoch:   2 [  6400/ 60000 ( 11%)]\tLoss: 0.039608\n",
      "Train Epoch:   2 [ 12800/ 60000 ( 21%)]\tLoss: 0.032125\n",
      "Train Epoch:   2 [ 19200/ 60000 ( 32%)]\tLoss: 0.001233\n",
      "Train Epoch:   2 [ 25600/ 60000 ( 43%)]\tLoss: 0.013566\n",
      "Train Epoch:   2 [ 32000/ 60000 ( 53%)]\tLoss: 0.002213\n",
      "Train Epoch:   2 [ 38400/ 60000 ( 64%)]\tLoss: 0.020720\n",
      "Train Epoch:   2 [ 44800/ 60000 ( 75%)]\tLoss: 0.005425\n",
      "Train Epoch:   2 [ 51200/ 60000 ( 85%)]\tLoss: 0.042364\n",
      "Train Epoch:   2 [ 57600/ 60000 ( 96%)]\tLoss: 0.033868\n",
      "Train Epoch:   3 [     0/ 60000 (  0%)]\tLoss: 0.001591\n",
      "Train Epoch:   3 [  6400/ 60000 ( 11%)]\tLoss: 0.002597\n",
      "Train Epoch:   3 [ 12800/ 60000 ( 21%)]\tLoss: 0.000523\n",
      "Train Epoch:   3 [ 19200/ 60000 ( 32%)]\tLoss: 0.022320\n",
      "Train Epoch:   3 [ 25600/ 60000 ( 43%)]\tLoss: 0.000972\n",
      "Train Epoch:   3 [ 32000/ 60000 ( 53%)]\tLoss: 0.001144\n",
      "Train Epoch:   3 [ 38400/ 60000 ( 64%)]\tLoss: 0.008388\n",
      "Train Epoch:   3 [ 44800/ 60000 ( 75%)]\tLoss: 0.002824\n",
      "Train Epoch:   3 [ 51200/ 60000 ( 85%)]\tLoss: 0.003181\n",
      "Train Epoch:   3 [ 57600/ 60000 ( 96%)]\tLoss: 0.004607\n",
      "Train Epoch:   4 [     0/ 60000 (  0%)]\tLoss: 0.000820\n",
      "Train Epoch:   4 [  6400/ 60000 ( 11%)]\tLoss: 0.002368\n",
      "Train Epoch:   4 [ 12800/ 60000 ( 21%)]\tLoss: 0.001305\n",
      "Train Epoch:   4 [ 19200/ 60000 ( 32%)]\tLoss: 0.000160\n",
      "Train Epoch:   4 [ 25600/ 60000 ( 43%)]\tLoss: 0.000631\n",
      "Train Epoch:   4 [ 32000/ 60000 ( 53%)]\tLoss: 0.005547\n",
      "Train Epoch:   4 [ 38400/ 60000 ( 64%)]\tLoss: 0.000257\n",
      "Train Epoch:   4 [ 44800/ 60000 ( 75%)]\tLoss: 0.000988\n",
      "Train Epoch:   4 [ 51200/ 60000 ( 85%)]\tLoss: 0.000162\n",
      "Train Epoch:   4 [ 57600/ 60000 ( 96%)]\tLoss: 0.037084\n",
      "Train Epoch:   5 [     0/ 60000 (  0%)]\tLoss: 0.002779\n",
      "Train Epoch:   5 [  6400/ 60000 ( 11%)]\tLoss: 0.000395\n",
      "Train Epoch:   5 [ 12800/ 60000 ( 21%)]\tLoss: 0.000784\n",
      "Train Epoch:   5 [ 19200/ 60000 ( 32%)]\tLoss: 0.002023\n",
      "Train Epoch:   5 [ 25600/ 60000 ( 43%)]\tLoss: 0.000427\n",
      "Train Epoch:   5 [ 32000/ 60000 ( 53%)]\tLoss: 0.047056\n",
      "Train Epoch:   5 [ 38400/ 60000 ( 64%)]\tLoss: 0.003164\n",
      "Train Epoch:   5 [ 44800/ 60000 ( 75%)]\tLoss: 0.002819\n",
      "Train Epoch:   5 [ 51200/ 60000 ( 85%)]\tLoss: 0.003611\n",
      "Train Epoch:   5 [ 57600/ 60000 ( 96%)]\tLoss: 0.000118\n",
      "Train Epoch:   6 [     0/ 60000 (  0%)]\tLoss: 0.001733\n",
      "Train Epoch:   6 [  6400/ 60000 ( 11%)]\tLoss: 0.002642\n",
      "Train Epoch:   6 [ 12800/ 60000 ( 21%)]\tLoss: 0.001134\n",
      "Train Epoch:   6 [ 19200/ 60000 ( 32%)]\tLoss: 0.010011\n",
      "Train Epoch:   6 [ 25600/ 60000 ( 43%)]\tLoss: 0.000209\n",
      "Train Epoch:   6 [ 32000/ 60000 ( 53%)]\tLoss: 0.000750\n",
      "Train Epoch:   6 [ 38400/ 60000 ( 64%)]\tLoss: 0.000426\n",
      "Train Epoch:   6 [ 44800/ 60000 ( 75%)]\tLoss: 0.000388\n",
      "Train Epoch:   6 [ 51200/ 60000 ( 85%)]\tLoss: 0.016945\n",
      "Train Epoch:   6 [ 57600/ 60000 ( 96%)]\tLoss: 0.000510\n",
      "Train Epoch:   7 [     0/ 60000 (  0%)]\tLoss: 0.000488\n",
      "Train Epoch:   7 [  6400/ 60000 ( 11%)]\tLoss: 0.001013\n",
      "Train Epoch:   7 [ 12800/ 60000 ( 21%)]\tLoss: 0.000112\n",
      "Train Epoch:   7 [ 19200/ 60000 ( 32%)]\tLoss: 0.000492\n",
      "Train Epoch:   7 [ 25600/ 60000 ( 43%)]\tLoss: 0.000041\n",
      "Train Epoch:   7 [ 32000/ 60000 ( 53%)]\tLoss: 0.000625\n",
      "Train Epoch:   7 [ 38400/ 60000 ( 64%)]\tLoss: 0.000025\n",
      "Train Epoch:   7 [ 44800/ 60000 ( 75%)]\tLoss: 0.000306\n",
      "Train Epoch:   7 [ 51200/ 60000 ( 85%)]\tLoss: 0.000895\n",
      "Train Epoch:   7 [ 57600/ 60000 ( 96%)]\tLoss: 0.000423\n",
      "Train Epoch:   8 [     0/ 60000 (  0%)]\tLoss: 0.000138\n",
      "Train Epoch:   8 [  6400/ 60000 ( 11%)]\tLoss: 0.000202\n",
      "Train Epoch:   8 [ 12800/ 60000 ( 21%)]\tLoss: 0.001193\n",
      "Train Epoch:   8 [ 19200/ 60000 ( 32%)]\tLoss: 0.000439\n",
      "Train Epoch:   8 [ 25600/ 60000 ( 43%)]\tLoss: 0.001685\n",
      "Train Epoch:   8 [ 32000/ 60000 ( 53%)]\tLoss: 0.001813\n",
      "Train Epoch:   8 [ 38400/ 60000 ( 64%)]\tLoss: 0.004245\n",
      "Train Epoch:   8 [ 44800/ 60000 ( 75%)]\tLoss: 0.000474\n",
      "Train Epoch:   8 [ 51200/ 60000 ( 85%)]\tLoss: 0.000050\n",
      "Train Epoch:   8 [ 57600/ 60000 ( 96%)]\tLoss: 0.000929\n",
      "Train Epoch:   9 [     0/ 60000 (  0%)]\tLoss: 0.000183\n",
      "Train Epoch:   9 [  6400/ 60000 ( 11%)]\tLoss: 0.000974\n",
      "Train Epoch:   9 [ 12800/ 60000 ( 21%)]\tLoss: 0.000143\n",
      "Train Epoch:   9 [ 19200/ 60000 ( 32%)]\tLoss: 0.000525\n",
      "Train Epoch:   9 [ 25600/ 60000 ( 43%)]\tLoss: 0.003575\n",
      "Train Epoch:   9 [ 32000/ 60000 ( 53%)]\tLoss: 0.001121\n",
      "Train Epoch:   9 [ 38400/ 60000 ( 64%)]\tLoss: 0.000051\n",
      "Train Epoch:   9 [ 44800/ 60000 ( 75%)]\tLoss: 0.000197\n",
      "Train Epoch:   9 [ 51200/ 60000 ( 85%)]\tLoss: 0.000006\n",
      "Train Epoch:   9 [ 57600/ 60000 ( 96%)]\tLoss: 0.000019\n",
      "Train Epoch:  10 [     0/ 60000 (  0%)]\tLoss: 0.000083\n",
      "Train Epoch:  10 [  6400/ 60000 ( 11%)]\tLoss: 0.000229\n",
      "Train Epoch:  10 [ 12800/ 60000 ( 21%)]\tLoss: 0.000011\n",
      "Train Epoch:  10 [ 19200/ 60000 ( 32%)]\tLoss: 0.000014\n",
      "Train Epoch:  10 [ 25600/ 60000 ( 43%)]\tLoss: 0.000024\n",
      "Train Epoch:  10 [ 32000/ 60000 ( 53%)]\tLoss: 0.000203\n",
      "Train Epoch:  10 [ 38400/ 60000 ( 64%)]\tLoss: 0.000231\n",
      "Train Epoch:  10 [ 44800/ 60000 ( 75%)]\tLoss: 0.000015\n",
      "Train Epoch:  10 [ 51200/ 60000 ( 85%)]\tLoss: 0.000008\n",
      "Train Epoch:  10 [ 57600/ 60000 ( 96%)]\tLoss: 0.000188\n",
      "Train Epoch:  11 [     0/ 60000 (  0%)]\tLoss: 0.000023\n",
      "Train Epoch:  11 [  6400/ 60000 ( 11%)]\tLoss: 0.000253\n",
      "Train Epoch:  11 [ 12800/ 60000 ( 21%)]\tLoss: 0.000183\n",
      "Train Epoch:  11 [ 19200/ 60000 ( 32%)]\tLoss: 0.000385\n",
      "Train Epoch:  11 [ 25600/ 60000 ( 43%)]\tLoss: 0.000007\n",
      "Train Epoch:  11 [ 32000/ 60000 ( 53%)]\tLoss: 0.000056\n",
      "Train Epoch:  11 [ 38400/ 60000 ( 64%)]\tLoss: 0.000009\n",
      "Train Epoch:  11 [ 44800/ 60000 ( 75%)]\tLoss: 0.000008\n",
      "Train Epoch:  11 [ 51200/ 60000 ( 85%)]\tLoss: 0.000727\n",
      "Train Epoch:  11 [ 57600/ 60000 ( 96%)]\tLoss: 0.000028\n",
      "Train Epoch:  12 [     0/ 60000 (  0%)]\tLoss: 0.000595\n",
      "Train Epoch:  12 [  6400/ 60000 ( 11%)]\tLoss: 0.000009\n",
      "Train Epoch:  12 [ 12800/ 60000 ( 21%)]\tLoss: 0.000199\n",
      "Train Epoch:  12 [ 19200/ 60000 ( 32%)]\tLoss: 0.000085\n",
      "Train Epoch:  12 [ 25600/ 60000 ( 43%)]\tLoss: 0.000125\n",
      "Train Epoch:  12 [ 32000/ 60000 ( 53%)]\tLoss: 0.000047\n",
      "Train Epoch:  12 [ 38400/ 60000 ( 64%)]\tLoss: 0.000823\n",
      "Train Epoch:  12 [ 44800/ 60000 ( 75%)]\tLoss: 0.000835\n",
      "Train Epoch:  12 [ 51200/ 60000 ( 85%)]\tLoss: 0.000041\n",
      "Train Epoch:  12 [ 57600/ 60000 ( 96%)]\tLoss: 0.000019\n",
      "Train Epoch:  13 [     0/ 60000 (  0%)]\tLoss: 0.000019\n",
      "Train Epoch:  13 [  6400/ 60000 ( 11%)]\tLoss: 0.000147\n",
      "Train Epoch:  13 [ 12800/ 60000 ( 21%)]\tLoss: 0.000054\n",
      "Train Epoch:  13 [ 19200/ 60000 ( 32%)]\tLoss: 0.000071\n",
      "Train Epoch:  13 [ 25600/ 60000 ( 43%)]\tLoss: 0.000343\n",
      "Train Epoch:  13 [ 32000/ 60000 ( 53%)]\tLoss: 0.000023\n",
      "Train Epoch:  13 [ 38400/ 60000 ( 64%)]\tLoss: 0.000234\n",
      "Train Epoch:  13 [ 44800/ 60000 ( 75%)]\tLoss: 0.000214\n",
      "Train Epoch:  13 [ 51200/ 60000 ( 85%)]\tLoss: 0.000016\n",
      "Train Epoch:  13 [ 57600/ 60000 ( 96%)]\tLoss: 0.000133\n",
      "Train Epoch:  14 [     0/ 60000 (  0%)]\tLoss: 0.000159\n",
      "Train Epoch:  14 [  6400/ 60000 ( 11%)]\tLoss: 0.000004\n",
      "Train Epoch:  14 [ 12800/ 60000 ( 21%)]\tLoss: 0.000014\n",
      "Train Epoch:  14 [ 19200/ 60000 ( 32%)]\tLoss: 0.000013\n",
      "Train Epoch:  14 [ 25600/ 60000 ( 43%)]\tLoss: 0.000169\n",
      "Train Epoch:  14 [ 32000/ 60000 ( 53%)]\tLoss: 0.000139\n",
      "Train Epoch:  14 [ 38400/ 60000 ( 64%)]\tLoss: 0.000091\n",
      "Train Epoch:  14 [ 44800/ 60000 ( 75%)]\tLoss: 0.000149\n",
      "Train Epoch:  14 [ 51200/ 60000 ( 85%)]\tLoss: 0.000055\n",
      "Train Epoch:  14 [ 57600/ 60000 ( 96%)]\tLoss: 0.000059\n",
      "Train Epoch:  15 [     0/ 60000 (  0%)]\tLoss: 0.000000\n",
      "Train Epoch:  15 [  6400/ 60000 ( 11%)]\tLoss: 0.000013\n",
      "Train Epoch:  15 [ 12800/ 60000 ( 21%)]\tLoss: 0.000001\n",
      "Train Epoch:  15 [ 19200/ 60000 ( 32%)]\tLoss: 0.000044\n",
      "Train Epoch:  15 [ 25600/ 60000 ( 43%)]\tLoss: 0.000016\n",
      "Train Epoch:  15 [ 32000/ 60000 ( 53%)]\tLoss: 0.000009\n",
      "Train Epoch:  15 [ 38400/ 60000 ( 64%)]\tLoss: 0.000143\n",
      "Train Epoch:  15 [ 44800/ 60000 ( 75%)]\tLoss: 0.000052\n",
      "Train Epoch:  15 [ 51200/ 60000 ( 85%)]\tLoss: 0.000031\n",
      "Train Epoch:  15 [ 57600/ 60000 ( 96%)]\tLoss: 0.000017\n",
      "Train Epoch:  16 [     0/ 60000 (  0%)]\tLoss: 0.000001\n",
      "Train Epoch:  16 [  6400/ 60000 ( 11%)]\tLoss: 0.000372\n",
      "Train Epoch:  16 [ 12800/ 60000 ( 21%)]\tLoss: 0.000091\n",
      "Train Epoch:  16 [ 19200/ 60000 ( 32%)]\tLoss: 0.000011\n",
      "Train Epoch:  16 [ 25600/ 60000 ( 43%)]\tLoss: 0.000367\n",
      "Train Epoch:  16 [ 32000/ 60000 ( 53%)]\tLoss: 0.000011\n",
      "Train Epoch:  16 [ 38400/ 60000 ( 64%)]\tLoss: 0.000290\n",
      "Train Epoch:  16 [ 44800/ 60000 ( 75%)]\tLoss: 0.000014\n",
      "Train Epoch:  16 [ 51200/ 60000 ( 85%)]\tLoss: 0.000034\n",
      "Train Epoch:  16 [ 57600/ 60000 ( 96%)]\tLoss: 0.000137\n",
      "Train Epoch:  17 [     0/ 60000 (  0%)]\tLoss: 0.000037\n",
      "Train Epoch:  17 [  6400/ 60000 ( 11%)]\tLoss: 0.000027\n",
      "Train Epoch:  17 [ 12800/ 60000 ( 21%)]\tLoss: 0.000081\n",
      "Train Epoch:  17 [ 19200/ 60000 ( 32%)]\tLoss: 0.000077\n",
      "Train Epoch:  17 [ 25600/ 60000 ( 43%)]\tLoss: 0.000171\n",
      "Train Epoch:  17 [ 32000/ 60000 ( 53%)]\tLoss: 0.000219\n",
      "Train Epoch:  17 [ 38400/ 60000 ( 64%)]\tLoss: 0.000011\n",
      "Train Epoch:  17 [ 44800/ 60000 ( 75%)]\tLoss: 0.000129\n",
      "Train Epoch:  17 [ 51200/ 60000 ( 85%)]\tLoss: 0.000015\n",
      "Train Epoch:  17 [ 57600/ 60000 ( 96%)]\tLoss: 0.000100\n",
      "Train Epoch:  18 [     0/ 60000 (  0%)]\tLoss: 0.000074\n",
      "Train Epoch:  18 [  6400/ 60000 ( 11%)]\tLoss: 0.000086\n",
      "Train Epoch:  18 [ 12800/ 60000 ( 21%)]\tLoss: 0.000237\n",
      "Train Epoch:  18 [ 19200/ 60000 ( 32%)]\tLoss: 0.000036\n",
      "Train Epoch:  18 [ 25600/ 60000 ( 43%)]\tLoss: 0.000158\n",
      "Train Epoch:  18 [ 32000/ 60000 ( 53%)]\tLoss: 0.000013\n",
      "Train Epoch:  18 [ 38400/ 60000 ( 64%)]\tLoss: 0.000082\n",
      "Train Epoch:  18 [ 44800/ 60000 ( 75%)]\tLoss: 0.000028\n",
      "Train Epoch:  18 [ 51200/ 60000 ( 85%)]\tLoss: 0.000015\n",
      "Train Epoch:  18 [ 57600/ 60000 ( 96%)]\tLoss: 0.000044\n",
      "Train Epoch:  19 [     0/ 60000 (  0%)]\tLoss: 0.000082\n",
      "Train Epoch:  19 [  6400/ 60000 ( 11%)]\tLoss: 0.000032\n",
      "Train Epoch:  19 [ 12800/ 60000 ( 21%)]\tLoss: 0.000006\n",
      "Train Epoch:  19 [ 19200/ 60000 ( 32%)]\tLoss: 0.000051\n",
      "Train Epoch:  19 [ 25600/ 60000 ( 43%)]\tLoss: 0.000005\n",
      "Train Epoch:  19 [ 32000/ 60000 ( 53%)]\tLoss: 0.000179\n",
      "Train Epoch:  19 [ 38400/ 60000 ( 64%)]\tLoss: 0.000009\n",
      "Train Epoch:  19 [ 44800/ 60000 ( 75%)]\tLoss: 0.000045\n",
      "Train Epoch:  19 [ 51200/ 60000 ( 85%)]\tLoss: 0.000050\n",
      "Train Epoch:  19 [ 57600/ 60000 ( 96%)]\tLoss: 0.000054\n",
      "Train Epoch:  20 [     0/ 60000 (  0%)]\tLoss: 0.000059\n",
      "Train Epoch:  20 [  6400/ 60000 ( 11%)]\tLoss: 0.000035\n",
      "Train Epoch:  20 [ 12800/ 60000 ( 21%)]\tLoss: 0.000016\n",
      "Train Epoch:  20 [ 19200/ 60000 ( 32%)]\tLoss: 0.000004\n",
      "Train Epoch:  20 [ 25600/ 60000 ( 43%)]\tLoss: 0.000078\n",
      "Train Epoch:  20 [ 32000/ 60000 ( 53%)]\tLoss: 0.000088\n",
      "Train Epoch:  20 [ 38400/ 60000 ( 64%)]\tLoss: 0.000153\n",
      "Train Epoch:  20 [ 44800/ 60000 ( 75%)]\tLoss: 0.000076\n",
      "Train Epoch:  20 [ 51200/ 60000 ( 85%)]\tLoss: 0.000099\n",
      "Train Epoch:  20 [ 57600/ 60000 ( 96%)]\tLoss: 0.000051\n",
      "Train Epoch:  21 [     0/ 60000 (  0%)]\tLoss: 0.000066\n",
      "Train Epoch:  21 [  6400/ 60000 ( 11%)]\tLoss: 0.000006\n",
      "Train Epoch:  21 [ 12800/ 60000 ( 21%)]\tLoss: 0.000004\n",
      "Train Epoch:  21 [ 19200/ 60000 ( 32%)]\tLoss: 0.000037\n",
      "Train Epoch:  21 [ 25600/ 60000 ( 43%)]\tLoss: 0.000010\n",
      "Train Epoch:  21 [ 32000/ 60000 ( 53%)]\tLoss: 0.000024\n",
      "Train Epoch:  21 [ 38400/ 60000 ( 64%)]\tLoss: 0.000028\n",
      "Train Epoch:  21 [ 44800/ 60000 ( 75%)]\tLoss: 0.000002\n",
      "Train Epoch:  21 [ 51200/ 60000 ( 85%)]\tLoss: 0.000287\n",
      "Train Epoch:  21 [ 57600/ 60000 ( 96%)]\tLoss: 0.000033\n",
      "Train Epoch:  22 [     0/ 60000 (  0%)]\tLoss: 0.000079\n",
      "Train Epoch:  22 [  6400/ 60000 ( 11%)]\tLoss: 0.000010\n",
      "Train Epoch:  22 [ 12800/ 60000 ( 21%)]\tLoss: 0.000003\n",
      "Train Epoch:  22 [ 19200/ 60000 ( 32%)]\tLoss: 0.000050\n",
      "Train Epoch:  22 [ 25600/ 60000 ( 43%)]\tLoss: 0.000092\n",
      "Train Epoch:  22 [ 32000/ 60000 ( 53%)]\tLoss: 0.000062\n",
      "Train Epoch:  22 [ 38400/ 60000 ( 64%)]\tLoss: 0.000028\n",
      "Train Epoch:  22 [ 44800/ 60000 ( 75%)]\tLoss: 0.000069\n",
      "Train Epoch:  22 [ 51200/ 60000 ( 85%)]\tLoss: 0.000004\n",
      "Train Epoch:  22 [ 57600/ 60000 ( 96%)]\tLoss: 0.000078\n",
      "Train Epoch:  23 [     0/ 60000 (  0%)]\tLoss: 0.000021\n",
      "Train Epoch:  23 [  6400/ 60000 ( 11%)]\tLoss: 0.000032\n",
      "Train Epoch:  23 [ 12800/ 60000 ( 21%)]\tLoss: 0.000004\n",
      "Train Epoch:  23 [ 19200/ 60000 ( 32%)]\tLoss: 0.000006\n",
      "Train Epoch:  23 [ 25600/ 60000 ( 43%)]\tLoss: 0.000025\n",
      "Train Epoch:  23 [ 32000/ 60000 ( 53%)]\tLoss: 0.000028\n",
      "Train Epoch:  23 [ 38400/ 60000 ( 64%)]\tLoss: 0.000007\n",
      "Train Epoch:  23 [ 44800/ 60000 ( 75%)]\tLoss: 0.000015\n",
      "Train Epoch:  23 [ 51200/ 60000 ( 85%)]\tLoss: 0.000054\n",
      "Train Epoch:  23 [ 57600/ 60000 ( 96%)]\tLoss: 0.000075\n",
      "Train Epoch:  24 [     0/ 60000 (  0%)]\tLoss: 0.000010\n",
      "Train Epoch:  24 [  6400/ 60000 ( 11%)]\tLoss: 0.000044\n",
      "Train Epoch:  24 [ 12800/ 60000 ( 21%)]\tLoss: 0.000005\n",
      "Train Epoch:  24 [ 19200/ 60000 ( 32%)]\tLoss: 0.000003\n",
      "Train Epoch:  24 [ 25600/ 60000 ( 43%)]\tLoss: 0.000001\n",
      "Train Epoch:  24 [ 32000/ 60000 ( 53%)]\tLoss: 0.000074\n",
      "Train Epoch:  24 [ 38400/ 60000 ( 64%)]\tLoss: 0.000029\n",
      "Train Epoch:  24 [ 44800/ 60000 ( 75%)]\tLoss: 0.000015\n",
      "Train Epoch:  24 [ 51200/ 60000 ( 85%)]\tLoss: 0.000021\n",
      "Train Epoch:  24 [ 57600/ 60000 ( 96%)]\tLoss: 0.000026\n",
      "Train Epoch:  25 [     0/ 60000 (  0%)]\tLoss: 0.000012\n",
      "Train Epoch:  25 [  6400/ 60000 ( 11%)]\tLoss: 0.000013\n",
      "Train Epoch:  25 [ 12800/ 60000 ( 21%)]\tLoss: 0.000002\n",
      "Train Epoch:  25 [ 19200/ 60000 ( 32%)]\tLoss: 0.000119\n",
      "Train Epoch:  25 [ 25600/ 60000 ( 43%)]\tLoss: 0.000151\n",
      "Train Epoch:  25 [ 32000/ 60000 ( 53%)]\tLoss: 0.000024\n",
      "Train Epoch:  25 [ 38400/ 60000 ( 64%)]\tLoss: 0.000065\n",
      "Train Epoch:  25 [ 44800/ 60000 ( 75%)]\tLoss: 0.000070\n",
      "Train Epoch:  25 [ 51200/ 60000 ( 85%)]\tLoss: 0.000007\n",
      "Train Epoch:  25 [ 57600/ 60000 ( 96%)]\tLoss: 0.000001\n",
      "Train Epoch:  26 [     0/ 60000 (  0%)]\tLoss: 0.000020\n",
      "Train Epoch:  26 [  6400/ 60000 ( 11%)]\tLoss: 0.000021\n",
      "Train Epoch:  26 [ 12800/ 60000 ( 21%)]\tLoss: 0.000003\n",
      "Train Epoch:  26 [ 19200/ 60000 ( 32%)]\tLoss: 0.000040\n",
      "Train Epoch:  26 [ 25600/ 60000 ( 43%)]\tLoss: 0.000032\n",
      "Train Epoch:  26 [ 32000/ 60000 ( 53%)]\tLoss: 0.000018\n",
      "Train Epoch:  26 [ 38400/ 60000 ( 64%)]\tLoss: 0.000000\n",
      "Train Epoch:  26 [ 44800/ 60000 ( 75%)]\tLoss: 0.000006\n",
      "Train Epoch:  26 [ 51200/ 60000 ( 85%)]\tLoss: 0.000060\n",
      "Train Epoch:  26 [ 57600/ 60000 ( 96%)]\tLoss: 0.000003\n",
      "Train Epoch:  27 [     0/ 60000 (  0%)]\tLoss: 0.000133\n",
      "Train Epoch:  27 [  6400/ 60000 ( 11%)]\tLoss: 0.000002\n",
      "Train Epoch:  27 [ 12800/ 60000 ( 21%)]\tLoss: 0.000009\n",
      "Train Epoch:  27 [ 19200/ 60000 ( 32%)]\tLoss: 0.000324\n",
      "Train Epoch:  27 [ 25600/ 60000 ( 43%)]\tLoss: 0.000590\n",
      "Train Epoch:  27 [ 32000/ 60000 ( 53%)]\tLoss: 0.000042\n",
      "Train Epoch:  27 [ 38400/ 60000 ( 64%)]\tLoss: 0.000088\n",
      "Train Epoch:  27 [ 44800/ 60000 ( 75%)]\tLoss: 0.000006\n",
      "Train Epoch:  27 [ 51200/ 60000 ( 85%)]\tLoss: 0.000057\n",
      "Train Epoch:  27 [ 57600/ 60000 ( 96%)]\tLoss: 0.000005\n",
      "Train Epoch:  28 [     0/ 60000 (  0%)]\tLoss: 0.000021\n",
      "Train Epoch:  28 [  6400/ 60000 ( 11%)]\tLoss: 0.000012\n",
      "Train Epoch:  28 [ 12800/ 60000 ( 21%)]\tLoss: 0.000018\n",
      "Train Epoch:  28 [ 19200/ 60000 ( 32%)]\tLoss: 0.000025\n",
      "Train Epoch:  28 [ 25600/ 60000 ( 43%)]\tLoss: 0.000033\n",
      "Train Epoch:  28 [ 32000/ 60000 ( 53%)]\tLoss: 0.000102\n",
      "Train Epoch:  28 [ 38400/ 60000 ( 64%)]\tLoss: 0.000000\n",
      "Train Epoch:  28 [ 44800/ 60000 ( 75%)]\tLoss: 0.000015\n",
      "Train Epoch:  28 [ 51200/ 60000 ( 85%)]\tLoss: 0.000046\n",
      "Train Epoch:  28 [ 57600/ 60000 ( 96%)]\tLoss: 0.000005\n",
      "Train Epoch:  29 [     0/ 60000 (  0%)]\tLoss: 0.000037\n",
      "Train Epoch:  29 [  6400/ 60000 ( 11%)]\tLoss: 0.000054\n",
      "Train Epoch:  29 [ 12800/ 60000 ( 21%)]\tLoss: 0.000019\n",
      "Train Epoch:  29 [ 19200/ 60000 ( 32%)]\tLoss: 0.000009\n",
      "Train Epoch:  29 [ 25600/ 60000 ( 43%)]\tLoss: 0.000001\n",
      "Train Epoch:  29 [ 32000/ 60000 ( 53%)]\tLoss: 0.000003\n",
      "Train Epoch:  29 [ 38400/ 60000 ( 64%)]\tLoss: 0.000012\n",
      "Train Epoch:  29 [ 44800/ 60000 ( 75%)]\tLoss: 0.000002\n",
      "Train Epoch:  29 [ 51200/ 60000 ( 85%)]\tLoss: 0.000016\n",
      "Train Epoch:  29 [ 57600/ 60000 ( 96%)]\tLoss: 0.000002\n",
      "Train Epoch:  30 [     0/ 60000 (  0%)]\tLoss: 0.000077\n",
      "Train Epoch:  30 [  6400/ 60000 ( 11%)]\tLoss: 0.000136\n",
      "Train Epoch:  30 [ 12800/ 60000 ( 21%)]\tLoss: 0.000006\n",
      "Train Epoch:  30 [ 19200/ 60000 ( 32%)]\tLoss: 0.000012\n",
      "Train Epoch:  30 [ 25600/ 60000 ( 43%)]\tLoss: 0.000001\n",
      "Train Epoch:  30 [ 32000/ 60000 ( 53%)]\tLoss: 0.000029\n",
      "Train Epoch:  30 [ 38400/ 60000 ( 64%)]\tLoss: 0.000001\n",
      "Train Epoch:  30 [ 44800/ 60000 ( 75%)]\tLoss: 0.000021\n",
      "Train Epoch:  30 [ 51200/ 60000 ( 85%)]\tLoss: 0.000144\n",
      "Train Epoch:  30 [ 57600/ 60000 ( 96%)]\tLoss: 0.000013\n",
      "Train Epoch:  31 [     0/ 60000 (  0%)]\tLoss: 0.000017\n",
      "Train Epoch:  31 [  6400/ 60000 ( 11%)]\tLoss: 0.000013\n",
      "Train Epoch:  31 [ 12800/ 60000 ( 21%)]\tLoss: 0.000001\n",
      "Train Epoch:  31 [ 19200/ 60000 ( 32%)]\tLoss: 0.000003\n",
      "Train Epoch:  31 [ 25600/ 60000 ( 43%)]\tLoss: 0.000016\n",
      "Train Epoch:  31 [ 32000/ 60000 ( 53%)]\tLoss: 0.000003\n",
      "Train Epoch:  31 [ 38400/ 60000 ( 64%)]\tLoss: 0.000031\n",
      "Train Epoch:  31 [ 44800/ 60000 ( 75%)]\tLoss: 0.000007\n",
      "Train Epoch:  31 [ 51200/ 60000 ( 85%)]\tLoss: 0.000007\n",
      "Train Epoch:  31 [ 57600/ 60000 ( 96%)]\tLoss: 0.000004\n",
      "Train Epoch:  32 [     0/ 60000 (  0%)]\tLoss: 0.000025\n",
      "Train Epoch:  32 [  6400/ 60000 ( 11%)]\tLoss: 0.000014\n",
      "Train Epoch:  32 [ 12800/ 60000 ( 21%)]\tLoss: 0.000022\n",
      "Train Epoch:  32 [ 19200/ 60000 ( 32%)]\tLoss: 0.000047\n",
      "Train Epoch:  32 [ 25600/ 60000 ( 43%)]\tLoss: 0.000038\n",
      "Train Epoch:  32 [ 32000/ 60000 ( 53%)]\tLoss: 0.000100\n",
      "Train Epoch:  32 [ 38400/ 60000 ( 64%)]\tLoss: 0.000017\n",
      "Train Epoch:  32 [ 44800/ 60000 ( 75%)]\tLoss: 0.000026\n",
      "Train Epoch:  32 [ 51200/ 60000 ( 85%)]\tLoss: 0.000002\n",
      "Train Epoch:  32 [ 57600/ 60000 ( 96%)]\tLoss: 0.000042\n",
      "Train Epoch:  33 [     0/ 60000 (  0%)]\tLoss: 0.000027\n",
      "Train Epoch:  33 [  6400/ 60000 ( 11%)]\tLoss: 0.000105\n",
      "Train Epoch:  33 [ 12800/ 60000 ( 21%)]\tLoss: 0.000008\n",
      "Train Epoch:  33 [ 19200/ 60000 ( 32%)]\tLoss: 0.000003\n",
      "Train Epoch:  33 [ 25600/ 60000 ( 43%)]\tLoss: 0.000005\n",
      "Train Epoch:  33 [ 32000/ 60000 ( 53%)]\tLoss: 0.000014\n",
      "Train Epoch:  33 [ 38400/ 60000 ( 64%)]\tLoss: 0.000008\n",
      "Train Epoch:  33 [ 44800/ 60000 ( 75%)]\tLoss: 0.000003\n",
      "Train Epoch:  33 [ 51200/ 60000 ( 85%)]\tLoss: 0.000004\n",
      "Train Epoch:  33 [ 57600/ 60000 ( 96%)]\tLoss: 0.000000\n",
      "Train Epoch:  34 [     0/ 60000 (  0%)]\tLoss: 0.000006\n",
      "Train Epoch:  34 [  6400/ 60000 ( 11%)]\tLoss: 0.000004\n",
      "Train Epoch:  34 [ 12800/ 60000 ( 21%)]\tLoss: 0.000008\n",
      "Train Epoch:  34 [ 19200/ 60000 ( 32%)]\tLoss: 0.000033\n",
      "Train Epoch:  34 [ 25600/ 60000 ( 43%)]\tLoss: 0.000007\n",
      "Train Epoch:  34 [ 32000/ 60000 ( 53%)]\tLoss: 0.000000\n",
      "Train Epoch:  34 [ 38400/ 60000 ( 64%)]\tLoss: 0.000003\n",
      "Train Epoch:  34 [ 44800/ 60000 ( 75%)]\tLoss: 0.000006\n",
      "Train Epoch:  34 [ 51200/ 60000 ( 85%)]\tLoss: 0.000001\n",
      "Train Epoch:  34 [ 57600/ 60000 ( 96%)]\tLoss: 0.000038\n",
      "Train Epoch:  35 [     0/ 60000 (  0%)]\tLoss: 0.000033\n",
      "Train Epoch:  35 [  6400/ 60000 ( 11%)]\tLoss: 0.000035\n",
      "Train Epoch:  35 [ 12800/ 60000 ( 21%)]\tLoss: 0.000060\n",
      "Train Epoch:  35 [ 19200/ 60000 ( 32%)]\tLoss: 0.000003\n",
      "Train Epoch:  35 [ 25600/ 60000 ( 43%)]\tLoss: 0.000004\n",
      "Train Epoch:  35 [ 32000/ 60000 ( 53%)]\tLoss: 0.000016\n",
      "Train Epoch:  35 [ 38400/ 60000 ( 64%)]\tLoss: 0.000015\n",
      "Train Epoch:  35 [ 44800/ 60000 ( 75%)]\tLoss: 0.000006\n",
      "Train Epoch:  35 [ 51200/ 60000 ( 85%)]\tLoss: 0.000002\n",
      "Train Epoch:  35 [ 57600/ 60000 ( 96%)]\tLoss: 0.000162\n",
      "Train Epoch:  36 [     0/ 60000 (  0%)]\tLoss: 0.000007\n",
      "Train Epoch:  36 [  6400/ 60000 ( 11%)]\tLoss: 0.000004\n",
      "Train Epoch:  36 [ 12800/ 60000 ( 21%)]\tLoss: 0.000025\n",
      "Train Epoch:  36 [ 19200/ 60000 ( 32%)]\tLoss: 0.000104\n",
      "Train Epoch:  36 [ 25600/ 60000 ( 43%)]\tLoss: 0.000005\n",
      "Train Epoch:  36 [ 32000/ 60000 ( 53%)]\tLoss: 0.000019\n",
      "Train Epoch:  36 [ 38400/ 60000 ( 64%)]\tLoss: 0.000050\n",
      "Train Epoch:  36 [ 44800/ 60000 ( 75%)]\tLoss: 0.000039\n",
      "Train Epoch:  36 [ 51200/ 60000 ( 85%)]\tLoss: 0.000055\n",
      "Train Epoch:  36 [ 57600/ 60000 ( 96%)]\tLoss: 0.000002\n",
      "Train Epoch:  37 [     0/ 60000 (  0%)]\tLoss: 0.000000\n",
      "Train Epoch:  37 [  6400/ 60000 ( 11%)]\tLoss: 0.000027\n",
      "Train Epoch:  37 [ 12800/ 60000 ( 21%)]\tLoss: 0.000016\n",
      "Train Epoch:  37 [ 19200/ 60000 ( 32%)]\tLoss: 0.000003\n",
      "Train Epoch:  37 [ 25600/ 60000 ( 43%)]\tLoss: 0.000003\n",
      "Train Epoch:  37 [ 32000/ 60000 ( 53%)]\tLoss: 0.000010\n",
      "Train Epoch:  37 [ 38400/ 60000 ( 64%)]\tLoss: 0.000004\n",
      "Train Epoch:  37 [ 44800/ 60000 ( 75%)]\tLoss: 0.000031\n",
      "Train Epoch:  37 [ 51200/ 60000 ( 85%)]\tLoss: 0.000026\n",
      "Train Epoch:  37 [ 57600/ 60000 ( 96%)]\tLoss: 0.000020\n",
      "Train Epoch:  38 [     0/ 60000 (  0%)]\tLoss: 0.000003\n",
      "Train Epoch:  38 [  6400/ 60000 ( 11%)]\tLoss: 0.000047\n",
      "Train Epoch:  38 [ 12800/ 60000 ( 21%)]\tLoss: 0.000009\n",
      "Train Epoch:  38 [ 19200/ 60000 ( 32%)]\tLoss: 0.000012\n",
      "Train Epoch:  38 [ 25600/ 60000 ( 43%)]\tLoss: 0.000011\n",
      "Train Epoch:  38 [ 32000/ 60000 ( 53%)]\tLoss: 0.000017\n",
      "Train Epoch:  38 [ 38400/ 60000 ( 64%)]\tLoss: 0.000108\n",
      "Train Epoch:  38 [ 44800/ 60000 ( 75%)]\tLoss: 0.000001\n",
      "Train Epoch:  38 [ 51200/ 60000 ( 85%)]\tLoss: 0.000018\n",
      "Train Epoch:  38 [ 57600/ 60000 ( 96%)]\tLoss: 0.000067\n",
      "Train Epoch:  39 [     0/ 60000 (  0%)]\tLoss: 0.000001\n",
      "Train Epoch:  39 [  6400/ 60000 ( 11%)]\tLoss: 0.000010\n",
      "Train Epoch:  39 [ 12800/ 60000 ( 21%)]\tLoss: 0.000001\n",
      "Train Epoch:  39 [ 19200/ 60000 ( 32%)]\tLoss: 0.000036\n",
      "Train Epoch:  39 [ 25600/ 60000 ( 43%)]\tLoss: 0.000010\n",
      "Train Epoch:  39 [ 32000/ 60000 ( 53%)]\tLoss: 0.000015\n",
      "Train Epoch:  39 [ 38400/ 60000 ( 64%)]\tLoss: 0.000015\n",
      "Train Epoch:  39 [ 44800/ 60000 ( 75%)]\tLoss: 0.000015\n",
      "Train Epoch:  39 [ 51200/ 60000 ( 85%)]\tLoss: 0.000014\n",
      "Train Epoch:  39 [ 57600/ 60000 ( 96%)]\tLoss: 0.000004\n",
      "Train Epoch:  40 [     0/ 60000 (  0%)]\tLoss: 0.000048\n",
      "Train Epoch:  40 [  6400/ 60000 ( 11%)]\tLoss: 0.000015\n",
      "Train Epoch:  40 [ 12800/ 60000 ( 21%)]\tLoss: 0.000009\n",
      "Train Epoch:  40 [ 19200/ 60000 ( 32%)]\tLoss: 0.000032\n",
      "Train Epoch:  40 [ 25600/ 60000 ( 43%)]\tLoss: 0.000012\n",
      "Train Epoch:  40 [ 32000/ 60000 ( 53%)]\tLoss: 0.000003\n",
      "Train Epoch:  40 [ 38400/ 60000 ( 64%)]\tLoss: 0.000043\n",
      "Train Epoch:  40 [ 44800/ 60000 ( 75%)]\tLoss: 0.000008\n",
      "Train Epoch:  40 [ 51200/ 60000 ( 85%)]\tLoss: 0.000007\n",
      "Train Epoch:  40 [ 57600/ 60000 ( 96%)]\tLoss: 0.000025\n",
      "Train Epoch:  41 [     0/ 60000 (  0%)]\tLoss: 0.000004\n",
      "Train Epoch:  41 [  6400/ 60000 ( 11%)]\tLoss: 0.000011\n",
      "Train Epoch:  41 [ 12800/ 60000 ( 21%)]\tLoss: 0.000019\n",
      "Train Epoch:  41 [ 19200/ 60000 ( 32%)]\tLoss: 0.000037\n",
      "Train Epoch:  41 [ 25600/ 60000 ( 43%)]\tLoss: 0.000012\n",
      "Train Epoch:  41 [ 32000/ 60000 ( 53%)]\tLoss: 0.000012\n",
      "Train Epoch:  41 [ 38400/ 60000 ( 64%)]\tLoss: 0.000027\n",
      "Train Epoch:  41 [ 44800/ 60000 ( 75%)]\tLoss: 0.000043\n",
      "Train Epoch:  41 [ 51200/ 60000 ( 85%)]\tLoss: 0.000009\n",
      "Train Epoch:  41 [ 57600/ 60000 ( 96%)]\tLoss: 0.000012\n",
      "Train Epoch:  42 [     0/ 60000 (  0%)]\tLoss: 0.000001\n",
      "Train Epoch:  42 [  6400/ 60000 ( 11%)]\tLoss: 0.000014\n",
      "Train Epoch:  42 [ 12800/ 60000 ( 21%)]\tLoss: 0.000004\n",
      "Train Epoch:  42 [ 19200/ 60000 ( 32%)]\tLoss: 0.000005\n",
      "Train Epoch:  42 [ 25600/ 60000 ( 43%)]\tLoss: 0.000001\n",
      "Train Epoch:  42 [ 32000/ 60000 ( 53%)]\tLoss: 0.000007\n",
      "Train Epoch:  42 [ 38400/ 60000 ( 64%)]\tLoss: 0.000002\n",
      "Train Epoch:  42 [ 44800/ 60000 ( 75%)]\tLoss: 0.000032\n",
      "Train Epoch:  42 [ 51200/ 60000 ( 85%)]\tLoss: 0.000002\n",
      "Train Epoch:  42 [ 57600/ 60000 ( 96%)]\tLoss: 0.000002\n",
      "Train Epoch:  43 [     0/ 60000 (  0%)]\tLoss: 0.000006\n",
      "Train Epoch:  43 [  6400/ 60000 ( 11%)]\tLoss: 0.000000\n",
      "Train Epoch:  43 [ 12800/ 60000 ( 21%)]\tLoss: 0.000002\n",
      "Train Epoch:  43 [ 19200/ 60000 ( 32%)]\tLoss: 0.000013\n",
      "Train Epoch:  43 [ 25600/ 60000 ( 43%)]\tLoss: 0.000006\n",
      "Train Epoch:  43 [ 32000/ 60000 ( 53%)]\tLoss: 0.000000\n",
      "Train Epoch:  43 [ 38400/ 60000 ( 64%)]\tLoss: 0.000028\n",
      "Train Epoch:  43 [ 44800/ 60000 ( 75%)]\tLoss: 0.000009\n",
      "Train Epoch:  43 [ 51200/ 60000 ( 85%)]\tLoss: 0.000017\n",
      "Train Epoch:  43 [ 57600/ 60000 ( 96%)]\tLoss: 0.000008\n",
      "Train Epoch:  44 [     0/ 60000 (  0%)]\tLoss: 0.000000\n",
      "Train Epoch:  44 [  6400/ 60000 ( 11%)]\tLoss: 0.000001\n",
      "Train Epoch:  44 [ 12800/ 60000 ( 21%)]\tLoss: 0.000017\n",
      "Train Epoch:  44 [ 19200/ 60000 ( 32%)]\tLoss: 0.000171\n",
      "Train Epoch:  44 [ 25600/ 60000 ( 43%)]\tLoss: 0.000001\n",
      "Train Epoch:  44 [ 32000/ 60000 ( 53%)]\tLoss: 0.000006\n",
      "Train Epoch:  44 [ 38400/ 60000 ( 64%)]\tLoss: 0.000007\n",
      "Train Epoch:  44 [ 44800/ 60000 ( 75%)]\tLoss: 0.000152\n",
      "Train Epoch:  44 [ 51200/ 60000 ( 85%)]\tLoss: 0.000149\n",
      "Train Epoch:  44 [ 57600/ 60000 ( 96%)]\tLoss: 0.000022\n",
      "Train Epoch:  45 [     0/ 60000 (  0%)]\tLoss: 0.000012\n",
      "Train Epoch:  45 [  6400/ 60000 ( 11%)]\tLoss: 0.000027\n",
      "Train Epoch:  45 [ 12800/ 60000 ( 21%)]\tLoss: 0.000062\n",
      "Train Epoch:  45 [ 19200/ 60000 ( 32%)]\tLoss: 0.000011\n",
      "Train Epoch:  45 [ 25600/ 60000 ( 43%)]\tLoss: 0.000015\n",
      "Train Epoch:  45 [ 32000/ 60000 ( 53%)]\tLoss: 0.000008\n",
      "Train Epoch:  45 [ 38400/ 60000 ( 64%)]\tLoss: 0.000020\n",
      "Train Epoch:  45 [ 44800/ 60000 ( 75%)]\tLoss: 0.000010\n",
      "Train Epoch:  45 [ 51200/ 60000 ( 85%)]\tLoss: 0.000024\n",
      "Train Epoch:  45 [ 57600/ 60000 ( 96%)]\tLoss: 0.000019\n",
      "Train Epoch:  46 [     0/ 60000 (  0%)]\tLoss: 0.000000\n",
      "Train Epoch:  46 [  6400/ 60000 ( 11%)]\tLoss: 0.000005\n",
      "Train Epoch:  46 [ 12800/ 60000 ( 21%)]\tLoss: 0.000163\n",
      "Train Epoch:  46 [ 19200/ 60000 ( 32%)]\tLoss: 0.000070\n",
      "Train Epoch:  46 [ 25600/ 60000 ( 43%)]\tLoss: 0.000016\n",
      "Train Epoch:  46 [ 32000/ 60000 ( 53%)]\tLoss: 0.000011\n",
      "Train Epoch:  46 [ 38400/ 60000 ( 64%)]\tLoss: 0.000017\n",
      "Train Epoch:  46 [ 44800/ 60000 ( 75%)]\tLoss: 0.000012\n",
      "Train Epoch:  46 [ 51200/ 60000 ( 85%)]\tLoss: 0.000048\n",
      "Train Epoch:  46 [ 57600/ 60000 ( 96%)]\tLoss: 0.000004\n",
      "Train Epoch:  47 [     0/ 60000 (  0%)]\tLoss: 0.000047\n",
      "Train Epoch:  47 [  6400/ 60000 ( 11%)]\tLoss: 0.000005\n",
      "Train Epoch:  47 [ 12800/ 60000 ( 21%)]\tLoss: 0.000049\n",
      "Train Epoch:  47 [ 19200/ 60000 ( 32%)]\tLoss: 0.000012\n",
      "Train Epoch:  47 [ 25600/ 60000 ( 43%)]\tLoss: 0.000025\n",
      "Train Epoch:  47 [ 32000/ 60000 ( 53%)]\tLoss: 0.000001\n",
      "Train Epoch:  47 [ 38400/ 60000 ( 64%)]\tLoss: 0.000004\n",
      "Train Epoch:  47 [ 44800/ 60000 ( 75%)]\tLoss: 0.000005\n",
      "Train Epoch:  47 [ 51200/ 60000 ( 85%)]\tLoss: 0.000001\n",
      "Train Epoch:  47 [ 57600/ 60000 ( 96%)]\tLoss: 0.000044\n",
      "Train Epoch:  48 [     0/ 60000 (  0%)]\tLoss: 0.000051\n",
      "Train Epoch:  48 [  6400/ 60000 ( 11%)]\tLoss: 0.000005\n",
      "Train Epoch:  48 [ 12800/ 60000 ( 21%)]\tLoss: 0.000031\n",
      "Train Epoch:  48 [ 19200/ 60000 ( 32%)]\tLoss: 0.000030\n",
      "Train Epoch:  48 [ 25600/ 60000 ( 43%)]\tLoss: 0.000001\n",
      "Train Epoch:  48 [ 32000/ 60000 ( 53%)]\tLoss: 0.000013\n",
      "Train Epoch:  48 [ 38400/ 60000 ( 64%)]\tLoss: 0.000006\n",
      "Train Epoch:  48 [ 44800/ 60000 ( 75%)]\tLoss: 0.000003\n",
      "Train Epoch:  48 [ 51200/ 60000 ( 85%)]\tLoss: 0.000006\n",
      "Train Epoch:  48 [ 57600/ 60000 ( 96%)]\tLoss: 0.000007\n",
      "Train Epoch:  49 [     0/ 60000 (  0%)]\tLoss: 0.000000\n",
      "Train Epoch:  49 [  6400/ 60000 ( 11%)]\tLoss: 0.000004\n",
      "Train Epoch:  49 [ 12800/ 60000 ( 21%)]\tLoss: 0.000001\n",
      "Train Epoch:  49 [ 19200/ 60000 ( 32%)]\tLoss: 0.000000\n",
      "Train Epoch:  49 [ 25600/ 60000 ( 43%)]\tLoss: 0.000001\n",
      "Train Epoch:  49 [ 32000/ 60000 ( 53%)]\tLoss: 0.000003\n",
      "Train Epoch:  49 [ 38400/ 60000 ( 64%)]\tLoss: 0.000037\n",
      "Train Epoch:  49 [ 44800/ 60000 ( 75%)]\tLoss: 0.000010\n",
      "Train Epoch:  49 [ 51200/ 60000 ( 85%)]\tLoss: 0.000012\n",
      "Train Epoch:  49 [ 57600/ 60000 ( 96%)]\tLoss: 0.000010\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from complexPyTorch.complexLayers import ComplexBatchNorm2d, ComplexConv2d, ComplexLinear\n",
    "from complexPyTorch.complexFunctions import complex_relu, complex_max_pool2d\n",
    "\n",
    "batch_size = 64\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "train_set = datasets.MNIST('../data', train=True, transform=trans, download=True)\n",
    "test_set = datasets.MNIST('../data', train=False, transform=trans, download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size= batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size= batch_size, shuffle=True)\n",
    "\n",
    "class ComplexNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ComplexNet, self).__init__()\n",
    "        self.conv1 = ComplexConv2d(1, 10, 5, 1)\n",
    "        self.bn  = ComplexBatchNorm2d(10)\n",
    "        self.conv2 = ComplexConv2d(10, 20, 5, 1)\n",
    "        self.fc1 = ComplexLinear(4*4*20, 500)\n",
    "        self.fc2 = ComplexLinear(500, 10)\n",
    "             \n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = complex_relu(x)\n",
    "        x = complex_max_pool2d(x, 2, 2)\n",
    "        x = self.bn(x)\n",
    "        x = self.conv2(x)\n",
    "        x = complex_relu(x)\n",
    "        x = complex_max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1,4*4*20)\n",
    "        x = self.fc1(x)\n",
    "        x = complex_relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = x.abs()\n",
    "        x =  F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ComplexNet().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device).type(torch.complex64), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {:3} [{:6}/{:6} ({:3.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch,\n",
    "                batch_idx * len(data), \n",
    "                len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), \n",
    "                loss.item())\n",
    "            )\n",
    "\n",
    "# Run training on 50 epochs\n",
    "for epoch in range(50):\n",
    "    train(model, device, train_loader, optimizer, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n",
      "155\n",
      "255\n"
     ]
    }
   ],
   "source": [
    "sum  = 0\n",
    "for i in range(11):\n",
    "    sum+=i\n",
    "print(sum)\n",
    "\n",
    "sum  = 0\n",
    "for i in range(11, 21):\n",
    "    sum+=i\n",
    "print(sum)\n",
    "\n",
    "sum = 0\n",
    "for i in range(21, 31):\n",
    "    sum+=i\n",
    "print(sum)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
