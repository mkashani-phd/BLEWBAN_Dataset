{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, pickle\n",
    "import numpy as np\n",
    "import pymongo\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import src.IQ as IQ\n",
    "\n",
    "\n",
    "myclient = pymongo.MongoClient(\"mongodb://127.0.0.1:27017/admin\")\n",
    "BLE = myclient[\"BLE\"]\n",
    "\n",
    "def query(collection, filter:dict, addFrameColumn=True):\n",
    "    df =  pd.DataFrame(list(collection.find(filter)))\n",
    "    if addFrameColumn:\n",
    "        df['frame'] = df.apply(lambda x: x['I'] + np.dot(x['Q'],1j), axis=1)\n",
    "    return df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-16 00:10:35.135861: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-16 00:10:35.159166: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-16 00:10:35.159189: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-16 00:10:35.159207: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-16 00:10:35.163858: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/moh/Documents/PhD/BLEWBAN_Dataset/Examples/auto_encoder.ipynb Cell 2\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/moh/Documents/PhD/BLEWBAN_Dataset/Examples/auto_encoder.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/moh/Documents/PhD/BLEWBAN_Dataset/Examples/auto_encoder.ipynb#W1sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcallbacks\u001b[39;00m \u001b[39mimport\u001b[39;00m EarlyStopping\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/moh/Documents/PhD/BLEWBAN_Dataset/Examples/auto_encoder.ipynb#W1sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m StandardScaler\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/moh/Documents/PhD/BLEWBAN_Dataset/Examples/auto_encoder.ipynb#W1sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend\u001b[39;00m \u001b[39mimport\u001b[39;00m clear_session\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/moh/Documents/PhD/BLEWBAN_Dataset/Examples/auto_encoder.ipynb#W1sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mregularizers\u001b[39;00m \u001b[39mimport\u001b[39;00m l1, l2\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/__init__.py:82\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _distributor_init  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m __check_build  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m clone\n\u001b[1;32m     83\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_show_versions\u001b[39;00m \u001b[39mimport\u001b[39;00m show_versions\n\u001b[1;32m     85\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[1;32m     86\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcalibration\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     87\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcluster\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mshow_versions\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    129\u001b[0m ]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m __version__\n\u001b[1;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_config\u001b[39;00m \u001b[39mimport\u001b[39;00m get_config\n\u001b[0;32m---> 17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m _IS_32BIT\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_set_output\u001b[39;00m \u001b[39mimport\u001b[39;00m _SetOutputMixin\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_tags\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     20\u001b[0m     _DEFAULT_TAGS,\n\u001b[1;32m     21\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/__init__.py:21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmurmurhash\u001b[39;00m \u001b[39mimport\u001b[39;00m murmurhash3_32\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mclass_weight\u001b[39;00m \u001b[39mimport\u001b[39;00m compute_class_weight, compute_sample_weight\n\u001b[0;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _joblib\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m DataConversionWarning\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdeprecation\u001b[39;00m \u001b[39mimport\u001b[39;00m deprecated\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_joblib.py:7\u001b[0m\n\u001b[1;32m      4\u001b[0m _warnings\u001b[39m.\u001b[39msimplefilter(\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[39m# joblib imports may raise DeprecationWarning on certain Python\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m# versions\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjoblib\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mjoblib\u001b[39;00m \u001b[39mimport\u001b[39;00m logger\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mjoblib\u001b[39;00m \u001b[39mimport\u001b[39;00m dump, load\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/__init__.py:113\u001b[0m\n\u001b[1;32m    109\u001b[0m __version__ \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m1.2.0\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    112\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmemory\u001b[39;00m \u001b[39mimport\u001b[39;00m Memory, MemorizedResult, register_store_backend\n\u001b[1;32m    114\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mlogger\u001b[39;00m \u001b[39mimport\u001b[39;00m PrintTime\n\u001b[1;32m    115\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mlogger\u001b[39;00m \u001b[39mimport\u001b[39;00m Logger\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/memory.py:32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfunc_inspect\u001b[39;00m \u001b[39mimport\u001b[39;00m format_signature\n\u001b[1;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mlogger\u001b[39;00m \u001b[39mimport\u001b[39;00m Logger, format_time, pformat\n\u001b[0;32m---> 32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_store_backends\u001b[39;00m \u001b[39mimport\u001b[39;00m StoreBackendBase, FileSystemStoreBackend\n\u001b[1;32m     35\u001b[0m FIRST_LINE_TEXT \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m# first line:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     37\u001b[0m \u001b[39m# TODO: The following object should have a data store object as a sub\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[39m# object, and the interface to persist and query should be separated in\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39m# the data store.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39m# TODO: Same remark for the logger, and probably use the Python logging\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[39m# mechanism.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/_store_backends.py:17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbackports\u001b[39;00m \u001b[39mimport\u001b[39;00m concurrency_safe_rename\n\u001b[1;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdisk\u001b[39;00m \u001b[39mimport\u001b[39;00m mkdirp, memstr_to_bytes, rm_subdirs\n\u001b[0;32m---> 17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m numpy_pickle\n\u001b[1;32m     19\u001b[0m CacheItemInfo \u001b[39m=\u001b[39m collections\u001b[39m.\u001b[39mnamedtuple(\u001b[39m'\u001b[39m\u001b[39mCacheItemInfo\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     20\u001b[0m                                        \u001b[39m'\u001b[39m\u001b[39mpath size last_access\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconcurrency_safe_write\u001b[39m(object_to_write, filename, write_func):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/numpy_pickle.py:13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mio\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpathlib\u001b[39;00m \u001b[39mimport\u001b[39;00m Path\n\u001b[0;32m---> 13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcompressor\u001b[39;00m \u001b[39mimport\u001b[39;00m lz4, LZ4_NOT_INSTALLED_ERROR\n\u001b[1;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcompressor\u001b[39;00m \u001b[39mimport\u001b[39;00m _COMPRESSORS, register_compressor, BinaryZlibFile\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcompressor\u001b[39;00m \u001b[39mimport\u001b[39;00m (ZlibCompressorWrapper, GzipCompressorWrapper,\n\u001b[1;32m     16\u001b[0m                          BZ2CompressorWrapper, LZMACompressorWrapper,\n\u001b[1;32m     17\u001b[0m                          XZCompressorWrapper, LZ4CompressorWrapper)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/compressor.py:231\u001b[0m\n\u001b[1;32m    227\u001b[0m _MODE_WRITE \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n\u001b[1;32m    228\u001b[0m _BUFFER_SIZE \u001b[39m=\u001b[39m \u001b[39m8192\u001b[39m\n\u001b[0;32m--> 231\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mBinaryZlibFile\u001b[39;00m(io\u001b[39m.\u001b[39mBufferedIOBase):\n\u001b[1;32m    232\u001b[0m     \u001b[39m\"\"\"A file object providing transparent zlib (de)compression.\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \n\u001b[1;32m    234\u001b[0m \u001b[39m    TODO python2_drop: is it still needed since we dropped Python 2 support A\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[39m    compression, and 9 produces the most compression. 3 is the default.\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    255\u001b[0m     wbits \u001b[39m=\u001b[39m zlib\u001b[39m.\u001b[39mMAX_WBITS\n",
      "File \u001b[0;32m/usr/lib/python3.10/abc.py:105\u001b[0m, in \u001b[0;36mABCMeta.__new__\u001b[0;34m(mcls, name, bases, namespace, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mABCMeta\u001b[39;00m(\u001b[39mtype\u001b[39m):\n\u001b[1;32m     93\u001b[0m     \u001b[39m\"\"\"Metaclass for defining Abstract Base Classes (ABCs).\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \n\u001b[1;32m     95\u001b[0m \u001b[39m    Use this metaclass to create an ABC.  An ABC can be subclassed\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39m    even via super()).\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__new__\u001b[39m(mcls, name, bases, namespace, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    106\u001b[0m         \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__new__\u001b[39m(mcls, name, bases, namespace, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    107\u001b[0m         _abc_init(\u001b[39mcls\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "import threading\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# We will start with a size that is approximately half of the input size\n",
    "def run(encoding_dim,input_tensor_features,output_tensor_features, plot = False):\n",
    "    tf.keras.backend.clear_session()\n",
    "    input_shape = input_tensor_features[0].shape[0]\n",
    "    output_shape = output_tensor_features[0].shape[0]\n",
    "    input_data = Input(shape=(input_shape,))\n",
    "    encoded = Dense(encoding_dim + (input_shape - encoding_dim )//2, activation='relu')(input_data)\n",
    "    encoded = Dense(encoding_dim + (input_shape - encoding_dim )//4, activation='tanh')(encoded)\n",
    "    \n",
    "    encoded = Dense(encoding_dim, activation='linear')(encoded) # kernel_regularizer=l1(0.01)\n",
    "\n",
    "\n",
    "    decoded = Dense(encoding_dim + (output_shape - encoding_dim )//4, activation='tanh')(encoded) \n",
    "    decoded = Dense(encoding_dim + (output_shape - encoding_dim )//2, activation='relu')(decoded)\n",
    "    decoded = Dense(output_shape, activation='linear')(decoded)\n",
    "\n",
    "    # This model maps an input to its reconstruction\n",
    "    autoencoder = Model(input_data, decoded)\n",
    "\n",
    "    # This model maps an input to its encoded representation\n",
    "    # encoder = Model(input_data, encoded)\n",
    "\n",
    "    # Compile the model\n",
    "    autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "    # Display the architecture of the autoencoder\n",
    "    # with lock:\n",
    "    #     autoencoder.summary()\n",
    "\n",
    "    # Train the autoencoder\n",
    "    # We will use new_csv_features as both the input and the target since it's an autoencoder\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=40, verbose=1)\n",
    "\n",
    "    # Train the autoencoder and save the history\n",
    "    history = autoencoder.fit(input_tensor_features, output_tensor_features,\n",
    "                            epochs=2048,\n",
    "                            batch_size=16,\n",
    "                            shuffle=True,\n",
    "                            validation_split=0.2,\n",
    "                            callbacks=[early_stopping],\n",
    "                            verbose=0,\n",
    "                            )  # Set verbose to 1 for progress output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    best_loss = min(history.history['val_loss']) # this is the raining loss\n",
    "    with lock:\n",
    "        try:\n",
    "            best_losses[encoding_dim] = min(best_loss,best_losses[encoding_dim])\n",
    "        except:\n",
    "            best_losses[encoding_dim] = best_loss\n",
    "            \n",
    "    print(\"latent dim:\", encoding_dim, \"loss value is: \", best_loss)\n",
    "\n",
    "    if plot:\n",
    "        lock.acquire()\n",
    "        plt.figure(figsize=(10, 2),dpi=80)\n",
    "        plt.plot(output_tensor_features[199], label='original')\n",
    "        plt.plot(autoencoder(input_tensor_features)[199], label='reconstructed')\n",
    "        plt.title('Latnet dim:'+ str(encoding_dim)+\", best loss:\"+str(best_loss)) \n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        # plt.figure(figsize=(20, 4))\n",
    "        # for i in range (1,len(autoencoder.layers)-1):\n",
    "        # # Assuming 'autoencoder' is your trained model and 'layer_index' is the index of the layer you want to analyze\n",
    "        #     weights = autoencoder.layers[i].get_weights()[0]  # 0 for weights, 1 for biases\n",
    "\n",
    "        #     # Plotting the weights as a heatmap\n",
    "            \n",
    "        #     plt.subplot(1,len(autoencoder.layers)-2,i)\n",
    "        #     plt.imshow(weights, cmap='viridis', aspect='auto')\n",
    "        #     plt.colorbar()\n",
    "\n",
    "        # plt.show()\n",
    "        # plt.close()\n",
    "\n",
    "        # plt.figure(figsize=(10, 16))\n",
    "        # for i in range (1,len(autoencoder.layers)-1):\n",
    "        # # Assuming 'autoencoder' is your trained model and 'layer_index' is the index of the layer you want to analyze\n",
    "        #     weights = autoencoder.layers[i].get_weights()[0]  # 0 for weights, 1 for biases\n",
    "        #     biases = autoencoder.layers[i].get_weights()[1]  # 0 for weights, 1 for biases\n",
    "        #     # Histogram of the weight values\n",
    "        #     plt.subplot(2*(len(autoencoder.layers)-2),1,i*2-1)\n",
    "        #     plt.plot(weights.flatten().tolist(), label='weights')\n",
    "        #     plt.plot(np.linspace(0,len(weights.flatten()),len(biases.flatten())),biases.flatten().tolist(), label='biases')\n",
    "        #     plt.legend()\n",
    "        #     plt.subplot(2*(len(autoencoder.layers)-2),2,i*4-1)\n",
    "        #     plt.hist(weights.flatten(), bins=50)\n",
    "\n",
    "            \n",
    "        #     # Histogram of the biases values\n",
    "        #     plt.subplot(2*(len(autoencoder.layers)-2),2,i*4)\n",
    "        #     plt.hist(biases.flatten(), bins=50)\n",
    "\n",
    "        # plt.show()\n",
    "        # plt.close()\n",
    "\n",
    "        lock.release()\n",
    "    tf.keras.backend.clear_session()\n",
    "    clear_session()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "scaler2 = MinMaxScaler(feature_range=(0, 1))\n",
    "# Assuming 'new_csv_features' is your data\n",
    "def normalized(row):\n",
    "    row = np.array(row)\n",
    "    return scaler2.fit_transform(scaler.fit_transform(row.reshape(-1, 1)))\n",
    "\n",
    "def fft_normalized(row):\n",
    "    row = np.array(row)\n",
    "    temp = np.fft.fft(row)[0:len(row)//2 + 1]\n",
    "\n",
    "    amp = normalized(np.abs(temp))\n",
    "    filtering = amp > np.average(amp)*.05\n",
    "    angle = normalized(np.angle(temp))\n",
    "\n",
    "    angle = angle[filtering] \n",
    "    angle = np.concatenate([angle,np.zeros(len(row)-len(angle))])\n",
    "    amp = amp[filtering] \n",
    "    amp = np.concatenate([amp,np.zeros(len(row)-len(amp))])\n",
    "    \n",
    "    return np.concatenate([amp,angle])\n",
    "\n",
    "lock = threading.Lock()\n",
    "\n",
    "\n",
    "filtering = {''}\n",
    "df = query(BLE['onBody'], {'pos':'static','antenna_side':'left'})\n",
    "min_length = df['frame'].apply(len).min()\n",
    "df['frame'] = df['frame'].apply(lambda x: x[:2000])\n",
    "print(len(df['frame'][0]))\n",
    "print(type(df['frame'][0]))\n",
    "\n",
    "iq = IQ.IQ(Fc=2439810000+.1e4)\n",
    "\n",
    "def configCreator(downSampleRate = 1, cutoff = 1e6):\n",
    "    downSampleRate= max(downSampleRate, 1)\n",
    "    return {                                      \n",
    "            # iq.gradient:{},\n",
    "            iq.unwrapPhase:{},\n",
    "            iq.phase:{}, \n",
    "            # iq.butter:{'Fs': iq.Fs/downSampleRate, \"cutoff\": cutoff},\n",
    "            iq.downSample:{'downSampleRate':downSampleRate, \"shift\": 0},\n",
    "            iq.demodulate:{'Fs': iq.Fs},\n",
    "           } \n",
    "\n",
    "methods = configCreator(downSampleRate=  4)\n",
    "df['data'] = iq.apply(methods = methods, frame = df)\n",
    "####################################################################################\n",
    "# df['normalized'] = df['predictor'].apply(lambda x: normalized(x))\n",
    "\n",
    "df['normalized'] = df['data'].apply(lambda x: fft_normalized(x))\n",
    "input_tensor_features = tf.convert_to_tensor(df['normalized'].tolist())\n",
    "\n",
    "df['normalized'] = df['data']#.apply(lambda x: normalized(x))\n",
    "output_tensor_features = tf.convert_to_tensor(df['normalized'].tolist())\n",
    "\n",
    "data_shape = len(df['data'][0])\n",
    "#####################################################################################\n",
    "\n",
    "best_losses = {}\n",
    "batch_size = 3\n",
    "for batch in range(0,int(np.ceil(data_shape/batch_size))):\n",
    "    # try:\n",
    "    #     run(batch,output_tensor_features,output_tensor_features,True)\n",
    "    # except:\n",
    "    #     continue\n",
    "    threads = []\n",
    "    # for i in range(batch_size * batch+1, min(batch_size * batch+batch_size+1,data_shape)):\n",
    "    for i in range(batch_size): # runnig the code batch_size times in parallel to find the miunimum loss\n",
    "        threads.append(threading.Thread(target=run, args=(batch+1,output_tensor_features,output_tensor_features,False, )))\n",
    "        threads[-1].start()\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "\n",
    "best_losses = dict(sorted(best_losses.items()))\n",
    "lossDF = pd.DataFrame(best_losses.values(),index=best_losses.keys())\n",
    "lossDF.plot()\n",
    "lossDF.to_csv('loss.csv')\n",
    "plt.xlabel('Latent dimension')\n",
    "plt.ylabel('Best loss')\n",
    "plt.title('dataset: onBody static left')\n",
    "plt.savefig('res/'+\"onBody static left\"+'.png')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "row =np.random.rand(10) \n",
    "\n",
    "filename = files[3]\n",
    "print(filename)\n",
    "f = open(dataDir+filename, 'rb')\n",
    "data_tuples = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "df = pd.DataFrame(data_tuples, columns=['predictor', 'label'])\n",
    "\n",
    "def fft_normalized(row):\n",
    "    temp = np.fft.fft(row)[0:len(row)//2 + 1]\n",
    "    amp = normalized(np.abs(temp))\n",
    "    filtering = amp > np.max(amp)*0.05 \n",
    "\n",
    "    angle = normalized(np.angle(temp))\n",
    "    angle = angle[filtering]\n",
    "    amp = amp[filtering]\n",
    "\n",
    "    return np.concatenate([amp,angle])\n",
    "\n",
    "\n",
    "df['normalized'] = df['predictor'].apply(lambda x: fft_normalized(x))\n",
    "tf.convert_to_tensor( df['normalized'].to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_losses = dict(sorted(best_losses.items()))\n",
    "# lossDF = pd.DataFrame(best_losses.values(),index=best_losses.keys())\n",
    "\n",
    "def read_loss(file :str | os.DirEntry):\n",
    "    if type(file) is str:\n",
    "        if file.endswith(\".csv\"):\n",
    "            lossDF = pd.read_csv(file,index_col=0)\n",
    "            return lossDF\n",
    "    else:\n",
    "        if file.name.endswith(\".csv\"):\n",
    "            lossDF = pd.read_csv(file.path,index_col=0)\n",
    "            return lossDF\n",
    "\n",
    "def plot_loss(file):\n",
    "        read_loss(file)\n",
    "        lossDF.plot()\n",
    "        plt.xlabel('Latent dimension')\n",
    "        plt.ylabel('Best loss')\n",
    "        plt.title('dataset: '+file.name)\n",
    "        plt.savefig('res/'+file.name+'.png')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "# for file in os.scandir('res/RAE-_l2-0.001/'):\n",
    "df = read_loss(\"loss_dataset1.pkl.csv\")\n",
    "import scipy.signal as signal\n",
    "\n",
    "def smooth(x,window_len=11,window='hanning'):\n",
    "    if x.ndim != 1:\n",
    "        raise ValueError(\"smooth only accepts 1 dimension arrays.\")\n",
    "    if x.size < window_len:\n",
    "        raise ValueError(\"Input vector needs to be bigger than window size.\")\n",
    "    if window_len<3:\n",
    "        return x\n",
    "    if not window in ['flat', 'hanning', 'hamming', 'bartlett', 'blackman']:\n",
    "        raise ValueError(f\"Window is on of '{window}'\")\n",
    "    s=np.r_[x[window_len-1:0:-1],x,x[-2:-window_len-1:-1]]\n",
    "    #print(len(s))\n",
    "    if window == 'flat': #moving average\n",
    "        w=np.ones(window_len,'d')\n",
    "    else:\n",
    "        w=eval(f'np.{window}({window_len})')\n",
    "    y=np.convolve(w/w.sum(),s,mode='valid')\n",
    "    return y\n",
    "\n",
    "plt.plot(smooth(df['0'].to_numpy(),window_len=5, window='hamming'))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
